{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eJ3khFtkSE0g"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardofae/RL/blob/main/AT-02/02%20-%20MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gymnasium - criando um MDP\n",
        "\n",
        "Gymnasium é a biblioteca que substituiu a pioneira 'gym' para definição de ambientes (MDPs) de aprendizado por reforço. No restante do notebook, manteremos o termo \"interface gym\", embora \"interface gymnasium\" seja mais preciso.\n",
        "\n",
        "Com contribuições do \"Stable Baselines3 Tutorial - Creating a custom Gym environment\" - https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb\n",
        "\n",
        "## Introducão\n",
        "\n",
        "Este notebook é uma introdução à interface Gym para ambientes de aprendizado por reforço. A biblioteca Gymnasium fornece uma interface que permite a modelagem do ambiente de uma forma padronizada para avaliação e teste de algoritmos de aprendizado por reforço.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hSPuBhqcodKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Formato do ambiente\n",
        "\n",
        "Um MDP é composto por estados, ações, transição e recompensa. Para modelar um MDP no gymnasium, voce deve definir duas propriedades e os métodos da interface.\n",
        "\n",
        "### Propriedades\n",
        "\n",
        "As propriedades são:\n",
        "- `observation_space` contém que tipo de espaço gym (gym space: `Discrete`, `Box`, ...) e a forma da observação (e.g. matriz 4x3).\n",
        "- `action_space` também é um objeto tipo gym space, definindo o tipo de ação que pode ser feita.\n",
        "\n",
        "O melhor jeito de aprender sobre gym spaces é olhando o [código](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces). De qualquer forma, os principais tipos são::\n",
        "- `gym.spaces.Box`: Um espaço (possivelmente sem limites) em $R^n$. Especificamente, Box representa o produto cartesiano de n intervalos fechados. Cada intervalo pode ser do tipo [a, b], (-oo, b], [a, oo), or (-oo, oo). Por exemplo, um vetor 1D ou uma imagem podem ser descritas como Box.\n",
        "```python\n",
        "# Exemplo de imagem como entrada, cada canal tem o menor valor em 0 e o maior em 255\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```\n",
        "\n",
        "- `gym.spaces.Discrete`: um conjunto discreto $\\{ 0, 1, \\dots, n-1 \\}$, geralmente útil para definir ações ou espaços discretos como gridworld.\n",
        "  Exemplo: se você tem duas ações (esquerda e direita), você pode representar seu espaço de ações com `Discrete(2)`, e ao implementar os métodos do ambiente você faz a ação 0 representar \"esquerda\" e a 1 representar \"direita\".\n",
        "\n",
        "\n",
        "### Métodos\n",
        "\n",
        "Devem ser implementados 3 métodos obrigatórios e um opcional:\n",
        "\n",
        "* `reset(seed)` para (re)iniciar o ambiente. Deve retornar a observação/estado inicial para o agente e um dict com informação adicional (pode ser vazio). Recebe uma semente aleatória para usar caso haja aleatoriedade. É chamado sempre que um novo episódio for começar.\n",
        "* `step(action)` recebe a ação a ser realizada no ambiente. O método deve realizar a ação e retornar uma tupla: `observation, reward, terminated, truncated, info` contendo, respectivamente, a observação (estado atingido), a recompensa recebida, se o episodio terminou por atingir um estado terminal (terminated) ou se foi interrompido (truncado, e.g. limite de tentativas atingido) e um dict com informações adicionais (pode ser vazio).\n",
        "* `close()`: se precisar \"limpar\" alguma coisa ao fechar o ambiente\n",
        "* (opcional) `render(method)`: gera uma visualização do ambiente. A função recebe o método de renderização (string) e deve gerar a visualização apropriada.\n",
        "\n"
      ],
      "metadata": {
        "id": "QQOBnCBdopnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando um novo ambiente\n",
        "\n",
        "Abaixo há uma implementação de um ambiente extremamente simples. É um grid unidimensional onde o agente começa na posição mais à direita e \"vence\" se chegar na posição mais à esquerda."
      ],
      "metadata": {
        "id": "WnEZQcZXHF8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import gymnasium.spaces as spaces\n",
        "\n",
        "\n",
        "class GoLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left.\n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoLeftEnv, self).__init__()\n",
        "\n",
        "    # Size of the 1D-grid\n",
        "    self.grid_size = grid_size\n",
        "\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_pos = grid_size - 1\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space, here we use Discrete\n",
        "    # the agent can be in any position from 0 to grid_size-1\n",
        "    self.observation_space = spaces.Discrete(self.grid_size)\n",
        "\n",
        "  def reset(self, seed=None):\n",
        "    \"\"\"\n",
        "    Important: se observation_space é Box, a observação deve ser um array numpy\n",
        "    :return: Tuple[np.array, dict] (obs e info; info é sempre vazio)\n",
        "    \"\"\"\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_pos = self.grid_size - 1\n",
        "    obs = self.agent_pos #np.array([self.agent_pos]) #.astype(np.uint8)\n",
        "    info = {}\n",
        "    return obs, info\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    Implementa a dinâmica do MDP. No nosso caso, basta ajustar a posição do agente\n",
        "    \"\"\"\n",
        "    if action != self.LEFT and action != self.RIGHT:    #checks for validity\n",
        "      raise ValueError(f\"Invalid action={action}\")\n",
        "\n",
        "    # position decreases if we move left, and increases if we move right\n",
        "    increment = -1 if action == self.LEFT else 1\n",
        "    self.agent_pos += increment\n",
        "\n",
        "    # Account for the boundaries of the world (keeps the agent between 0 and grid_size-1)\n",
        "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size-1)\n",
        "\n",
        "    # The game \"finishes\" when the agent gets to the leftmost position\n",
        "    terminated = bool(self.agent_pos == 0)\n",
        "\n",
        "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "    reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "\n",
        "    # converts the state to a numpy array and returns the experience tuple\n",
        "    obs = self.agent_pos #np.array([self.agent_pos]) #.astype(np.float32)\n",
        "\n",
        "    # for now, the episode has no timestep limit, so, it's never truncated\n",
        "    truncated = False\n",
        "    return obs, reward, terminated, truncated, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    \"\"\"\n",
        "    'Desenha' o ambiente para visualização\n",
        "    \"\"\"\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    print(\".\" * self.agent_pos, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size-1 - self.agent_pos))\n",
        "\n",
        "\n",
        "  def close(self):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "hGQrhfboHOPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine"
      },
      "source": [
        "### Validar o ambiente\n",
        "\n",
        "Stable Baselines3 fornece um [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) pra verificar se o ambiente segue a interface Gym. Também checa se o ambiente é compativel com os ambientes do Stable-Baselines (e dá warning se precisar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DOpP_B0-LXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55cbe1f6-e52c-40a8-d862-d43677f9d76e"
      },
      "source": [
        "from gymnasium.utils.env_checker import check_env\n",
        "env = GoLeftEnv()\n",
        "\n",
        "try:\n",
        "    check_env(env)\n",
        "    print(\"Environment passes all checks!\")\n",
        "except Exception as e:\n",
        "    print(f\"Environment has issues: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment has issues: Expects the random number generator to have been generated given a seed was passed to reset. Most likely the environment reset function does not call `super().reset(seed=seed)`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g"
      },
      "source": [
        "### Testando o ambiente com um agente aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i62yf2LvSAYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c1369c-a215-438b-eea6-e7e00a11037e"
      },
      "source": [
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "obs, _ = env.reset(seed=None)\n",
        "print('Obs space: ', env.observation_space)\n",
        "print('Action space:', env.action_space)\n",
        "print(f\"Initial state: obs={obs}. Render:\".ljust(58), end=' ')\n",
        "env.render()\n",
        "\n",
        "for step in range(50):\n",
        "  print(f\"Step {step + 1}:\", end=' ')\n",
        "  action = env.action_space.sample()\n",
        "  obs, reward, term, trunc, info = env.step(action)\n",
        "  print(f'obs={obs}, rwd={reward}, trunc={trunc}, term={term}. Render:'.ljust(50), end=' ')\n",
        "  env.render()\n",
        "  if term:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obs space:  Discrete(10)\n",
            "Action space: Discrete(2)\n",
            "Initial state: obs=9. Render:                              .........x\n",
            "Step 1: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 2: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 3: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
            "Step 4: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 5: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
            "Step 6: obs=5, rwd=0, trunc=False, term=False. Render:     .....x....\n",
            "Step 7: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
            "Step 8: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 9: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
            "Step 10: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 11: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 12: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 13: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 14: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 15: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 16: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 17: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 18: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 19: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 20: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 21: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 22: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 23: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 24: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 25: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
            "Step 26: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 27: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 28: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 29: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 30: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 31: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 32: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 33: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 34: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 35: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 36: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 37: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 38: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 39: obs=9, rwd=0, trunc=False, term=False. Render:     .........x\n",
            "Step 40: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 41: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 42: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n",
            "Step 43: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 44: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
            "Step 45: obs=5, rwd=0, trunc=False, term=False. Render:     .....x....\n",
            "Step 46: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
            "Step 47: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 48: obs=6, rwd=0, trunc=False, term=False. Render:     ......x...\n",
            "Step 49: obs=7, rwd=0, trunc=False, term=False. Render:     .......x..\n",
            "Step 50: obs=8, rwd=0, trunc=False, term=False. Render:     ........x.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tarefa 1 - GoLeftEnv\n",
        "\n",
        "Implemente e execute um agente com a política ótima no GoLeftEnv"
      ],
      "metadata": {
        "id": "iQqNqBdpHfzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## seu codigo aqui, pode se basear no agente aleatorio\n",
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "obs, _ = env.reset(seed=None)\n",
        "print('Obs space: ', env.observation_space)\n",
        "print('Action space:', env.action_space)\n",
        "print(f\"Initial state: obs={obs}. Render:\".ljust(54), end=' ')\n",
        "env.render()\n",
        "\n",
        "step, term = 1, False\n",
        "while not term:\n",
        "  action = env.LEFT\n",
        "  obs, reward, term, trunc, info = env.step(action)\n",
        "  print(f'step={step}, obs={obs}, rwd={reward}, trunc={trunc}, term={term}. Render:'.ljust(50), end=' ')\n",
        "  env.render()\n",
        "  if term:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "  step+=1"
      ],
      "metadata": {
        "id": "mMf07N-xHrk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc243950-a30e-41d5-e4d1-eecd29cdb89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obs space:  Discrete(10)\n",
            "Action space: Discrete(2)\n",
            "Initial state: obs=9. Render:                          .........x\n",
            "step=1, obs=8, rwd=0, trunc=False, term=False. Render: ........x.\n",
            "step=2, obs=7, rwd=0, trunc=False, term=False. Render: .......x..\n",
            "step=3, obs=6, rwd=0, trunc=False, term=False. Render: ......x...\n",
            "step=4, obs=5, rwd=0, trunc=False, term=False. Render: .....x....\n",
            "step=5, obs=4, rwd=0, trunc=False, term=False. Render: ....x.....\n",
            "step=6, obs=3, rwd=0, trunc=False, term=False. Render: ...x......\n",
            "step=7, obs=2, rwd=0, trunc=False, term=False. Render: ..x.......\n",
            "step=8, obs=1, rwd=0, trunc=False, term=False. Render: .x........\n",
            "step=9, obs=0, rwd=1, trunc=False, term=True. Render: x.........\n",
            "Goal reached! reward= 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo GoLeftEnv\n",
        "\n",
        "Para um exemplo de como instanciar um agente em um ambiente Gym, veja:  https://colab.research.google.com/drive/1ihwBXcMuVhrxk_xZjmcyAN_AOoTmRq9C#scrollTo=EIxbc7IIXL_Q"
      ],
      "metadata": {
        "id": "nlQZBZ5ltK4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tarefa 2 - Grid 4x3\n",
        "\n",
        "Implemente o ambiente do grid 4x3, preenchendo as células abaixo. Você deve permitir ao usuário especificar a recompensa de cada passo (padrão = -0.04), a probabilidade de 'escorregar' (padrão = 0.2) e o numero máximo de passos antes de encerrar o episódio. O espaço de ações deve ser discreto, com 4 ações, e o de estados também será discreto, com 12 estados (mesmo o estado 'parede' pode ser considerado nesta contagem). A convenção para numeração dos estados é (G=goal, #=parede, P=pit/buraco,S=start):\n",
        "```\n",
        "y=2    +----+----+----+----+\n",
        "       |  8 |  9 | 10 | 11G|\n",
        "       +----+----+----+----+\n",
        "y=1    |  4 |  5#|  6 |  7P|\n",
        "       +----+----+----+----+\n",
        "y=0    |  0S|  1 |  2 |  3 |\n",
        "       +----+----+----+----+\n",
        "        x=0   x=1   x=2   x=3\n",
        "```\n",
        "\n",
        "Note que há métodos para converter a numeração de estados para as coordenadas x,y"
      ],
      "metadata": {
        "id": "3lf22cmCHwlr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlzbqKU-fTHm"
      },
      "outputs": [],
      "source": [
        "class GridWorld4x3(gym.Env):\n",
        "    \"\"\"\n",
        "    GridWorld 4x3 environment compatible with gymnasium.\n",
        "    Actions: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
        "    \"\"\"\n",
        "    metadata = {\"render_modes\": [\"human\"]}\n",
        "\n",
        "    UP = 0\n",
        "    RIGHT = 1\n",
        "    DOWN = 2\n",
        "    LEFT = 3\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        reward_step: float = -0.04,\n",
        "        slip: float = 0.2,\n",
        "        max_steps: int = 1000,\n",
        "        seed: int = None,\n",
        "        render_mode = \"human\"\n",
        "    ):\n",
        "        self.reward_step = reward_step\n",
        "        self.slip = slip\n",
        "        self.max_steps = max_steps\n",
        "        self.seed = seed\n",
        "\n",
        "        # Grid Description\n",
        "        self.grid_size = (4,3)\n",
        "        self.wall  = [5]\n",
        "        self.goal  = [11]\n",
        "        self.pit   = [7]\n",
        "        self.start = 0\n",
        "\n",
        "        # Rewards\n",
        "        self.goal_reward = 1\n",
        "        self.pit_reward = -1\n",
        "\n",
        "        # Agent Position and steps made\n",
        "        self.agent_state = self.start\n",
        "        self.steps = 0\n",
        "\n",
        "        # Definition of action space\n",
        "        n_actions = 4\n",
        "        self.action_space = spaces.Discrete(n_actions)\n",
        "\n",
        "        # Definition of observation space\n",
        "        self.observation_space = spaces.Discrete(self.grid_size[0]*self.grid_size[1])\n",
        "\n",
        "        # Update Render Mode\n",
        "        self.metadata = {\"render_modes\": [render_mode]}\n",
        "\n",
        "    # ======================\n",
        "    # Métodos de conversão\n",
        "    # ======================\n",
        "    def pos_to_state(self, pos: tuple[int,int]) -> int:\n",
        "        \"\"\"Converte (x,y) → estado\"\"\"\n",
        "        x, y = pos\n",
        "        return y * self.grid_size[0] + x\n",
        "\n",
        "    def state_to_pos(self, s: int) -> tuple[int,int]:\n",
        "        \"\"\"Converte estado → (x,y)\"\"\"\n",
        "        return (s % self.grid_size[0], s // self.grid_size[0])\n",
        "\n",
        "    # ======================\n",
        "    # Gym API\n",
        "    # ======================\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Reset agent position and step count\n",
        "        self.agent_state = self.start\n",
        "        self.steps = 0\n",
        "\n",
        "        # Returns\n",
        "        obs = self.agent_state\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def is_vertical(self, action: int):\n",
        "        return action == self.UP or action == self.DOWN\n",
        "\n",
        "    def handle_slip(self, action: int):\n",
        "        direction = 1 if np.random.rand() < 0.5 else -1\n",
        "        action = action + direction\n",
        "        return action%4 if action != -1 else 3\n",
        "\n",
        "    def get_reward(self):\n",
        "        if self.agent_state in self.goal:\n",
        "            return self.goal_reward\n",
        "        if self.agent_state in self.pit:\n",
        "            return self.pit_reward\n",
        "        return self.reward_step\n",
        "\n",
        "    def step(self, action: int):\n",
        "        # Get cur position\n",
        "        x, y = self.state_to_pos(self.agent_state)\n",
        "\n",
        "        # Handle slip\n",
        "        if np.random.rand() < 0.2:\n",
        "            action = self.handle_slip(action)\n",
        "\n",
        "        # Update position\n",
        "        if self.is_vertical(action):\n",
        "            new_y = np.clip(y+(1-action), 0, self.grid_size[1]-1)\n",
        "            if self.pos_to_state((x,new_y)) not in self.wall:\n",
        "                y = new_y\n",
        "        else:\n",
        "            new_x = np.clip(x+(2-action), 0, self.grid_size[0]-1)\n",
        "            if self.pos_to_state((new_x,y)) not in self.wall:\n",
        "                x = new_x\n",
        "        self.agent_state = self.pos_to_state((x, y))\n",
        "\n",
        "        # Checks if goal state reached\n",
        "        terminated = self.agent_state in self.goal\n",
        "\n",
        "        # Calculates the reward\n",
        "        reward = self.get_reward()\n",
        "\n",
        "        # Not using\n",
        "        info = {}\n",
        "\n",
        "        # Returns the agent state\n",
        "        obs = self.agent_state\n",
        "\n",
        "        # Ends the episode when max steps reached\n",
        "        truncated = self.steps >= self.max_steps\n",
        "        self.steps += 1\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "    # ======================\n",
        "    # Render\n",
        "    # ======================\n",
        "    def render(self, mode=None):\n",
        "        div = ' ' * 7 + '+----' * self.grid_size[0] + '+'\n",
        "        for i in range(self.grid_size[1]):\n",
        "            y = self.grid_size[1]-i-1\n",
        "            print(div)\n",
        "            print(f'y={y:<5}', end='')\n",
        "            for x in range(self.grid_size[0]):\n",
        "                state = self.pos_to_state((x,y))\n",
        "                state_class = 'G' if state in self.goal else '#' if state in self.wall else 'P' if state in self.pit else 'S' if state == self.start else ''\n",
        "                state_print = (str(state) if self.agent_state != state else 'X') + state_class\n",
        "                print(f'|{state_print:^4}', end='')\n",
        "            print('|')\n",
        "        print(div)\n",
        "        print('        ', end='')\n",
        "        for x in range(self.grid_size[0]):\n",
        "            print(f'x={x:<2} ', end='')\n",
        "\n",
        "    def close(self):\n",
        "        pass # nao precisa implementar\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.utils.env_checker import check_env\n",
        "\n",
        "# Criar uma instância do ambiente\n",
        "env = GridWorld4x3()\n",
        "\n",
        "# This will catch many common issues\n",
        "try:\n",
        "    check_env(env)\n",
        "    print(\"Environment passes all checks!\")\n",
        "except Exception as e:\n",
        "    print(f\"Environment has issues: {e}\")"
      ],
      "metadata": {
        "id": "MNfZO-RQgHU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82de5c0-3f19-487a-935f-8d71939da046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment passes all checks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Política simples no grid\n",
        "\n",
        "A politica abaixo tenta se mover para a direita até atingir uma parede, e então vai pra cima."
      ],
      "metadata": {
        "id": "AHUIBJyFkic9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67ddbbe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d286dd14-b0b1-4a0d-aa06-1226227248b3"
      },
      "source": [
        "# Create an instance of the environment\n",
        "env = GridWorld4x3(render_mode=\"human\")\n",
        "\n",
        "# Define a simple policy (e.g., move right until hitting a wall, then move up)\n",
        "# This is a list of actions: 1=RIGHT, 0=UP\n",
        "simple_policy_actions = [1, 1, 1, 0, 1, 1, 1, 0] # Example sequence\n",
        "\n",
        "obs, info = env.reset()\n",
        "env.render()\n",
        "done = False\n",
        "total_reward = 0.0\n",
        "step_count = 0\n",
        "\n",
        "print(\"\\nRunning episode with simple policy:\")\n",
        "\n",
        "for action in simple_policy_actions:\n",
        "    if done:\n",
        "        break\n",
        "    print(f\"\\nTaking action: {action}\")\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    done = terminated or truncated\n",
        "    env.render()\n",
        "    step_count += 1\n",
        "\n",
        "print(\"\\nEpisode finished.\")\n",
        "print(f\"Total reward: {total_reward}\")\n",
        "print(f\"Steps taken: {step_count}\")\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | XS | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Running episode with simple policy:\n",
            "\n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | X  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | X  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | X  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 0\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | XP |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | X  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | XP |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | XP |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 0\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 | XG |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Episode finished.\n",
            "Total reward: -2.16\n",
            "Steps taken: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tarefa 3: política melhorada\n",
        "\n",
        "Implemente uma política que você considere ótima, que mapeie qualquer estado possível para a melhor ação correspondente. A política não precisa ser realmente a ótima, mas você deve descrevê-la textualmente para vermos a consistência com o que está implementado"
      ],
      "metadata": {
        "id": "Z48UjgV3krnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the environment\n",
        "env = GridWorld4x3(render_mode=\"human\")\n",
        "\n",
        "# Política: Sempre anda pra cima até estar na linha do topo, então vai pra direita\n",
        "# Caso a parede ou o pit estejam acima, vai para a esquerda\n",
        "def my_police(state: int):\n",
        "    if state == 1 or state == 3:\n",
        "        return env.LEFT\n",
        "    if state < 8:\n",
        "        return env.UP\n",
        "    return env.RIGHT\n",
        "\n",
        "\n",
        "obs, info = env.reset()\n",
        "env.render()\n",
        "done = False\n",
        "total_reward = 0.0\n",
        "step_count = 0\n",
        "\n",
        "print(\"\\nRunning episode with simple policy:\")\n",
        "\n",
        "while not done:\n",
        "    action = my_police(obs)\n",
        "    print(f\"\\nTaking action: {action}\")\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    total_reward += reward\n",
        "    done = terminated or truncated\n",
        "    env.render()\n",
        "    step_count += 1\n",
        "\n",
        "print(\"\\nEpisode finished.\")\n",
        "print(f\"Total reward: {total_reward}\")\n",
        "print(f\"Steps taken: {step_count}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "NXHQcL8flTGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be7a8aff-8029-4f8c-b585-776a14ac440f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | XS | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Running episode with simple policy:\n",
            "\n",
            "Taking action: 0\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | X  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 0\n",
            "       +----+----+----+----+\n",
            "y=2    | X  | 9  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | X  | 10 |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | X  |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | X  |11G |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Taking action: 1\n",
            "       +----+----+----+----+\n",
            "y=2    | 8  | 9  | 10 | XG |\n",
            "       +----+----+----+----+\n",
            "y=1    | 4  | 5# | 6  | 7P |\n",
            "       +----+----+----+----+\n",
            "y=0    | 0S | 1  | 2  | 3  |\n",
            "       +----+----+----+----+\n",
            "        x=0  x=1  x=2  x=3  \n",
            "Episode finished.\n",
            "Total reward: 0.8\n",
            "Steps taken: 6\n"
          ]
        }
      ]
    }
  ]
}