{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6L9Xbw9jpArh",
        "CPRbrlPlq532",
        "awdoIzbcZbsY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardofae/RL/blob/main/AT-05/05%20-%20TD%20learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7a844f5"
      },
      "source": [
        "# Métodos de Diferença Temporal\n",
        "\n",
        "Nesta tarefa, você irá implementar e comparar os algoritmos de Aprendizado por Reforço por Diferença Temporal SARSA e Q-Learning.\n",
        "\n",
        "**Objetivos:**\n",
        "\n",
        "1.  **Compreender a Diferença entre SARSA e Q-Learning:** Observe e implemente as regras de atualização de Q-values para ambos os algoritmos, prestando atenção especial à forma como cada um calcula o alvo na atualização de valor.\n",
        "2.  **Implementar Agentes SARSA e Q-Learning:** Preencha as classes `SarsaAgent` e `QLearningAgent` para que implementem corretamente os algoritmos SARSA e Q-Learning, respectivamente. Certifique-se de que as funções `updateQ` e `train` estejam alinhadas com as especificações de cada algoritmo.\n",
        "3.  **Testar no Grid 4x3:** Utilize os agentes implementados para resolver o ambiente Grid 4x3 fornecido. Observe as políticas aprendidas e os valores Q resultantes.\n",
        "5.  **Testar no Cliff Walking:** Treine os agentes SARSA e Q-Learning no ambiente Cliff Walking.\n",
        "6.  **Comparar Políticas no Cliff Walking:** Analise e compare as políticas ótimas aprendidas por SARSA e Q-Learning no ambiente Cliff Walking. **Observe atentamente as diferenças nas rotas preferidas pelos agentes.** Devido à natureza on-policy do SARSA e off-policy do Q-Learning, suas políticas ótimas no Cliff Walking (um ambiente com penalidade grande por cair do precipício) deverão ser notavelmente diferentes. SARSA tenderá a aprender uma política mais \"segura\", enquanto Q-Learning pode aprender uma política que, embora ótima em termos de valor Q, é mais arriscada durante o treinamento sob uma política epsilon-gulosa.\n",
        "7.  **Visualizar Resultados:** Utilize o `AgentVisualizer` (adaptado para o ambiente Cliff Walking, se necessário) para visualizar as políticas e valores Q aprendidos em ambos os ambientes.\n",
        "\n",
        "Ao final desta tarefa, você deverá ser capaz de explicar a diferença fundamental entre SARSA e Q-Learning e demonstrar como essa diferença se manifesta nas políticas aprendidas em um ambiente com riscos como o Cliff Walking."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid 4x3\n",
        "\n",
        "O código abaixo deve ser preenchido com a implementação do grid 4x3, com as mesmas especificações do Colab de [MDPs](https://colab.research.google.com/drive/1iNG9EX1-piXQvmN4-wQzncBv4EX7-yZV?usp=sharing). Portanto, você pode/deve aproveitar o código já feito. Não há mais aquelas funções adicionais para permitir que o ambiente seja \"consultado\" para execução de métodos de programação dinâmica, porque agora o agente não vai conhecer a dinâmica do ambiente. Ele irá interagir somente pela API padrão do gymnasium.\n"
      ],
      "metadata": {
        "id": "3lf22cmCHwlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Especificação do GridWorld (igual à tarefa de MDPs)\n",
        "\n",
        "A célula abaixo tem a especificação do grid 4x3, igual no colab de MDPs. Pode pular se você já tiver feito."
      ],
      "metadata": {
        "id": "6L9Xbw9jpArh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Implemente o ambiente do grid 4x3, preenchendo as células abaixo. Você deve permitir ao usuário especificar a recompensa de cada passo (padrão = -0.04), a probabilidade de 'escorregar' (padrão = 0.2) e o numero máximo de passos antes de encerrar o episódio. O espaço de ações deve ser discreto, com 4 ações (0=UP, 1=RIGHT, 2=DOWN, 3=LEFT), e o de estados também será discreto, com 12 estados (mesmo o estado 'parede' pode ser considerado nesta contagem). A convenção para numeração dos estados é (G=goal, #=parede, P=pit/buraco,S=start):\n",
        "```\n",
        "y=2    +----+----+----+----+\n",
        "       |  8 |  9 | 10 | 11G|\n",
        "       +----+----+----+----+\n",
        "y=1    |  4 |  5#|  6 |  7P|\n",
        "       +----+----+----+----+\n",
        "y=0    |  0S|  1 |  2 |  3 |\n",
        "       +----+----+----+----+\n",
        "        x=0   x=1   x=2   x=3\n",
        "```\n",
        "\n",
        "Note que há métodos para converter a numeração de estados para as coordenadas x,y"
      ],
      "metadata": {
        "id": "1T3ssw9boxKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código do GridWorld 4x3"
      ],
      "metadata": {
        "id": "CPRbrlPlq532"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlzbqKU-fTHm"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "from typing import Optional,Iterable,Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class GridWorld4x3(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"ansi\"]}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        reward_step: float = -0.04,\n",
        "        slip: float = 0.2,\n",
        "        max_steps: int = 1000,\n",
        "        seed: Optional[int] = None,\n",
        "        render_mode: Optional[str] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ncols = 4\n",
        "        self.nrows = 3\n",
        "        self.observation_space = spaces.Discrete(self.ncols * self.nrows)\n",
        "        self.action_space = spaces.Discrete(4)  # 0=up, 1=right, 2=down, 3=left\n",
        "\n",
        "        self.reward_step = reward_step\n",
        "        self.slip = slip\n",
        "        self.max_steps = max_steps\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.start_pos = (0, 0)\n",
        "        self.goal_pos = (3, 2)  # state 11\n",
        "        self.pit_pos = (3, 1)   # state 7\n",
        "        self.wall_pos = (1, 1)  # state 5 (inacessível)\n",
        "\n",
        "        self._rng = np.random.default_rng(seed)\n",
        "        self.steps = 0\n",
        "        self.agent_pos = self.start_pos\n",
        "\n",
        "        # Movimentos: up, right, down, left\n",
        "        self.moves = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "\n",
        "    # ---------- conversão estado/posição ----------\n",
        "    def pos_to_state(self, pos):\n",
        "        x, y = pos\n",
        "        return y * self.ncols + x\n",
        "\n",
        "    def state_to_pos(self, s):\n",
        "        return (s % self.ncols, s // self.ncols)\n",
        "\n",
        "    # ---------- helpers internos ----------\n",
        "    def _move(self, pos, action):\n",
        "        dx, dy = self.moves[action]\n",
        "        x, y = pos\n",
        "        new_pos = (x + dx, y + dy)\n",
        "        # checa limites e parede\n",
        "        if not (0 <= new_pos[0] < self.ncols and 0 <= new_pos[1] < self.nrows):\n",
        "            return pos\n",
        "        if new_pos == self.wall_pos:\n",
        "            return pos\n",
        "        return new_pos\n",
        "\n",
        "    def _reward_and_done(self, pos):\n",
        "        if pos == self.goal_pos:\n",
        "            return 1.0, True\n",
        "        elif pos == self.pit_pos:\n",
        "            return -1.0, True\n",
        "        return self.reward_step, False\n",
        "\n",
        "    # ---------- API Gym ----------\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.steps = 0\n",
        "        return self.pos_to_state(self.agent_pos), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "\n",
        "        # sorteia se escorrega\n",
        "        if self._rng.random() < self.slip:\n",
        "            if action in [0, 2]:  # up/down → troca por left/right\n",
        "                action = self._rng.choice([1, 3])\n",
        "            else:  # left/right → troca por up/down\n",
        "                action = self._rng.choice([0, 2])\n",
        "\n",
        "        self.agent_pos = self._move(self.agent_pos, action)\n",
        "        reward, terminated = self._reward_and_done(self.agent_pos)\n",
        "        truncated = self.steps >= self.max_steps\n",
        "\n",
        "        return self.pos_to_state(self.agent_pos), reward, terminated, truncated, {}\n",
        "\n",
        "    # ----------------------------\n",
        "    # Rendering\n",
        "    # ----------------------------\n",
        "    def render(self, mode=\"ansi\"):\n",
        "        if mode == \"ansi\":\n",
        "            return self._render_ansi()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def _render_ansi(self):\n",
        "        out = \"\"\n",
        "        for y in reversed(range(self.nrows)):\n",
        "            out += \"+----\" * self.ncols + \"+\\n\"\n",
        "            for x in range(self.ncols):\n",
        "                pos = (x, y)\n",
        "                s = self.pos_to_state(pos)\n",
        "                cell = f\"{s:2d} \"\n",
        "                if pos == self.wall_pos:\n",
        "                    cell = \" ## \"\n",
        "                elif pos == self.goal_pos:\n",
        "                    cell = f\"{s:2d}G\"\n",
        "                elif pos == self.pit_pos:\n",
        "                    cell = f\"{s:2d}P\"\n",
        "                elif pos == self.start_pos:\n",
        "                    cell = f\"{s:2d}S\"\n",
        "                if self.agent_pos == self.state_to_pos(s):\n",
        "                    cell = f\"[{cell.strip()}]\"\n",
        "                out += f\"|{cell:4}\"\n",
        "            out += \"|\\n\"\n",
        "        out += \"+----\" * self.ncols + \"+\\n\"\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codigo pra testar o Env\n",
        "\n",
        "A célula abaixo faz o teste básico pra verificar que o ambiente respeita a interface gymnasium"
      ],
      "metadata": {
        "id": "awdoIzbcZbsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deve continuar passando nos testes do Gymnasium\n",
        "from gymnasium.utils.env_checker import check_env\n",
        "\n",
        "# Criar uma instância do ambiente\n",
        "env = GridWorld4x3()\n",
        "\n",
        "# This will catch many common issues\n",
        "try:\n",
        "    check_env(env)\n",
        "    print(\"Environment passes all checks!\")\n",
        "except Exception as e:\n",
        "    print(f\"Environment has issues: {e}\")"
      ],
      "metadata": {
        "id": "MNfZO-RQgHU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363fd4d9-65f6-485f-e05c-8ad21ea60b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment passes all checks!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ff6845"
      },
      "source": [
        "## Agente de TD learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora você irá implementar os algoritmos de Aprendizado por Reforço por Diferença Temporal (TD Learning). Você irá trabalhar em subclasses da classe base `TDAgent` fornecida abaixo. Seu objetivo é preencher as partes faltando para que os agentes SARSA e Q-Learning sejam capazes de escolher ações e atualizar estimativas de valor para encontrar a política ótima em um ambiente. No caso deste enunciado será o grid 4x3 e Cliff Walking.\n",
        "\n",
        "Você irá:\n",
        "\n",
        "1.  **Compreender a classe base `TDAgent`**: Analisar a classe base `TDAgent` para entender os métodos e atributos comuns aos agentes SARSA e Q-Learning.\n",
        "2.  **Implementar Agentes SARSA e Q-Learning:** Preencher as classes `SarsaAgent` e `QLearningAgent` (que já estão na sequência do notebook) para que implementem corretamente os algoritmos SARSA e Q-Learning, respectivamente. Certifique-se de que as funções `updateQ` e `train` estejam alinhadas com as especificações de cada algoritmo.\n"
      ],
      "metadata": {
        "id": "Wj0RIeObNy67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código do TDAgent"
      ],
      "metadata": {
        "id": "PIqB_priN9kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC,abstractmethod\n",
        "\n",
        "class TDAgent(ABC):\n",
        "    def __init__(self, env: gym.Env, alpha: float = 0.1, gamma: float = 0.99, epsilon: float = 0.1):\n",
        "        \"\"\"\n",
        "        Construtor do agente TD.\n",
        "\n",
        "        Args:\n",
        "            env: ambiente Gymnasium (ex: gridworld 4x3).\n",
        "            alpha: taxa de aprendizado.\n",
        "            gamma: fator de desconto.\n",
        "            epsilon: taxa de exploração (para política epsilon-greedy).\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        obs_space_size = env.observation_space.n\n",
        "        act_space_size = env.action_space.n\n",
        "        self.q_values = np.zeros((obs_space_size, act_space_size))\n",
        "\n",
        "    def Q(self, state, action) -> float:\n",
        "      \"\"\"Retorna Q(s,a).\"\"\"\n",
        "      return self.q_values[state, action]\n",
        "\n",
        "    def V(self, state) -> float:\n",
        "      \"\"\"Retorna V(s) = max_a Q(s,a).\"\"\"\n",
        "      return np.max(self.q_values[state, :])\n",
        "\n",
        "    def greedy_action(self, state) -> int:\n",
        "      \"\"\"Retorna a ação gulosa (argmax_a Q(s,a)).\"\"\"\n",
        "      return np.argmax(self.q_values[state, :])\n",
        "\n",
        "    def act(self, state) -> int:\n",
        "      \"\"\"Retorna ação epsilon-greedy.\"\"\"\n",
        "      if np.random.rand() < self.epsilon:\n",
        "          return self.env.action_space.sample()\n",
        "      else:\n",
        "          return self.greedy_action(state)\n",
        "\n",
        "    @abstractmethod\n",
        "    def train(self, steps: int):\n",
        "      \"\"\"\n",
        "      Executa o treinamento por um número de passos.\n",
        "\n",
        "      Args:\n",
        "          steps: número de passos de treino.\n",
        "      \"\"\"\n",
        "      pass\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S1bPltlYvOdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código do SarsaAgent"
      ],
      "metadata": {
        "id": "uykBj6UnOAwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SarsaAgent(TDAgent):\n",
        "    def updateQ(self, s, a, r, s_next, a_next, done: bool):\n",
        "        \"\"\"Atualiza Q(s,a) segundo a regra do SARSA.\"\"\"\n",
        "        self.q_values[s, a] += self.alpha * (r + self.gamma * self.Q(s_next, a_next) - self.Q(s, a))\n",
        "\n",
        "    def train(self, steps: int):\n",
        "        state, _ = self.env.reset()\n",
        "        action = self.act(state)\n",
        "        for step in range(steps):\n",
        "            new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            new_action = self.act(new_state)\n",
        "            done = terminated or truncated\n",
        "            self.updateQ(state, action, reward, new_state, new_action, done)\n",
        "            state = new_state\n",
        "            action = new_action\n",
        "            if done:\n",
        "                state, _ = self.env.reset()\n",
        "                action = self.act(state)\n"
      ],
      "metadata": {
        "id": "uGkLkeDKnyAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testes do SarsaAgent"
      ],
      "metadata": {
        "id": "H5Rv9bQyOEQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Testes para SARSA\n",
        "# ----------------------\n",
        "def test_sarsa_update():\n",
        "    class DummyEnv:\n",
        "        observation_space = type(\"obs\", (), {\"n\": 2})\n",
        "        action_space = type(\"act\", (), {\"n\": 2})\n",
        "    env = DummyEnv()\n",
        "\n",
        "    agent = SarsaAgent(env, alpha=0.5, gamma=1.0, epsilon=0.0)\n",
        "    agent.q_values[:] = 0.0\n",
        "\n",
        "    s, a, r, s_next, a_next = 0, 0, 1, 1, 1\n",
        "    agent.q_values[s_next, a_next] = 2.0  # valor da ação escolhida em s'\n",
        "\n",
        "    # target = r + γ Q(s',a') = 1 + 2 = 3\n",
        "    # update: 0 + 0.5*(3-0) = 1.5\n",
        "    agent.updateQ(s, a, r, s_next, a_next, done=False)\n",
        "\n",
        "    assert np.isclose(agent.q_values[s, a], 1.5)\n",
        "\n",
        "\n",
        "def test_sarsa_update_terminal():\n",
        "    class DummyEnv:\n",
        "        observation_space = type(\"obs\", (), {\"n\": 2})\n",
        "        action_space = type(\"act\", (), {\"n\": 2})\n",
        "    env = DummyEnv()\n",
        "\n",
        "    agent = SarsaAgent(env, alpha=1.0, gamma=1.0, epsilon=0.0)\n",
        "    agent.q_values[:] = 0.0\n",
        "\n",
        "    s, a, r, s_next, a_next = 0, 1, -1, 1, 0\n",
        "    agent.updateQ(s, a, r, s_next, a_next, done=True)\n",
        "\n",
        "    # Como done=True, target = r = -1\n",
        "    assert np.isclose(agent.q_values[s, a], -1.0)\n"
      ],
      "metadata": {
        "id": "pzec3jVMxhwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código do QLearningAgent"
      ],
      "metadata": {
        "id": "AyT12R1yOG93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent(TDAgent):\n",
        "    def updateQ(self, s, a, r, s_next, done: bool):\n",
        "        \"\"\"Atualiza Q(s,a) segundo a regra do Q-Learning.\"\"\"\n",
        "        self.q_values[s, a] += self.alpha * (r + self.gamma * self.Q(s_next, self.greedy_action(s_next)) - self.Q(s, a))\n",
        "\n",
        "    def train(self, steps: int):\n",
        "        state, _ = self.env.reset()\n",
        "        for step in range(steps):\n",
        "            action = self.act(state)\n",
        "            new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            self.updateQ(state, action, reward, new_state, done)\n",
        "            state = new_state\n",
        "            if done:\n",
        "                state, _ = self.env.reset()\n",
        "\n"
      ],
      "metadata": {
        "id": "7VNzIJ4RqRRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teste do Q0learningAgent"
      ],
      "metadata": {
        "id": "gTcTOukcOJKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Testes para Q-Learning\n",
        "# ----------------------\n",
        "def test_qlearning_update():\n",
        "    class DummyEnv:\n",
        "        observation_space = type(\"obs\", (), {\"n\": 2})\n",
        "        action_space = type(\"act\", (), {\"n\": 2})\n",
        "    env = DummyEnv()\n",
        "\n",
        "    agent = QLearningAgent(env, alpha=0.5, gamma=1.0, epsilon=0.0)\n",
        "    agent.q_values[:] = 0.0\n",
        "\n",
        "    s, a, r, s_next = 0, 0, 1, 1\n",
        "    agent.q_values[s_next, 1] = 2.0  # valor ótimo em s'\n",
        "\n",
        "    # target = r + γ max_a Q(s',a) = 1 + 2 = 3\n",
        "    # update: Q(s,a) += α * (target - Q(s,a)) = 0 + 0.5*(3-0) = 1.5\n",
        "    agent.updateQ(s, a, r, s_next, done=False)\n",
        "\n",
        "    assert np.isclose(agent.q_values[s, a], 1.5)\n",
        "\n",
        "\n",
        "def test_qlearning_update_terminal():\n",
        "    class DummyEnv:\n",
        "        observation_space = type(\"obs\", (), {\"n\": 2})\n",
        "        action_space = type(\"act\", (), {\"n\": 2})\n",
        "    env = DummyEnv()\n",
        "\n",
        "    agent = QLearningAgent(env, alpha=1.0, gamma=1.0, epsilon=0.0)\n",
        "    agent.q_values[:] = 0.0\n",
        "\n",
        "    s, a, r, s_next = 0, 1, -1, 1\n",
        "    agent.updateQ(s, a, r, s_next, done=True)\n",
        "\n",
        "    # Como done=True, target = r = -1\n",
        "    assert np.isclose(agent.q_values[s, a], -1.0)\n",
        "test_qlearning_update()\n",
        "test_qlearning_update_terminal()"
      ],
      "metadata": {
        "id": "eGuRh-laxlpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilitário para visualizar o agente no Grid 4x3\n",
        "\n",
        "O mesmo da tarefa anterior."
      ],
      "metadata": {
        "id": "vDWttyDnXZNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código do utilitário"
      ],
      "metadata": {
        "id": "6eV6pDHSZKDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentVisualizer:\n",
        "    def __init__(self, agent, env):\n",
        "        \"\"\"\n",
        "        agent: ValueIterationAgent-like (tem V(s), Q(s,a) e greedy_action(s))\n",
        "        env: GridWorld4x3-like (tem nrows, ncols, pos_to_state, state_to_pos, is_terminal, get_states, start_pos, goal_pos, pit_pos, wall_pos)\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.action_to_str = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\"}\n",
        "\n",
        "        # Precompute special states\n",
        "        self.wall_s = self.env.pos_to_state(self.env.wall_pos)\n",
        "        self.start_s = self.env.pos_to_state(self.env.start_pos)\n",
        "        self.goal_s = self.env.pos_to_state(self.env.goal_pos)\n",
        "        self.pit_s = self.env.pos_to_state(self.env.pit_pos)\n",
        "\n",
        "    # -----------------------\n",
        "    # Política (setas)\n",
        "    # -----------------------\n",
        "    def print_policy(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    content = \"##\"\n",
        "                elif s == self.goal_s:\n",
        "                    content = \" G \"\n",
        "                elif s == self.pit_s:\n",
        "                    content = \" P \"\n",
        "                else:\n",
        "                    a = self.agent.greedy_action(s)\n",
        "                    arrow = self.action_to_str.get(a, \"?\")\n",
        "                    if s == self.start_s:\n",
        "                        content = f\"S{arrow}\"\n",
        "                    else:\n",
        "                        content = arrow\n",
        "                cells.append(f\"{content:^6}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Valores V(s)\n",
        "    # -----------------------\n",
        "    def print_values(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"--------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    content = \"####\"\n",
        "                else:\n",
        "                    v = self.agent.V(s)\n",
        "                    if s == self.goal_s:\n",
        "                        content = f\"G({v:.2f})\"\n",
        "                    elif s == self.pit_s:\n",
        "                        content = f\"P({v:.2f})\"\n",
        "                    else:\n",
        "                        content = f\"{v:6.2f}\"\n",
        "                cells.append(f\"{content:^8}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Q-values\n",
        "    # -----------------------\n",
        "    def print_qvalues(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"---------------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            # três linhas por célula\n",
        "            line1, line2, line3 = [], [], []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    c1 = \"###############\"\n",
        "                    c2 = \"###############\"\n",
        "                    c3 = \"###############\"\n",
        "                else:\n",
        "                    qvals = [self.agent.Q(s, a) for a in range(4)]\n",
        "                    best = int(np.argmax(qvals))\n",
        "                    up = f\"↑:{qvals[0]:.2f}\"\n",
        "                    left = f\"←:{qvals[3]:.2f}\"\n",
        "                    right = f\"→:{qvals[1]:.2f}\"\n",
        "                    down = f\"↓:{qvals[2]:.2f}\"\n",
        "                    c1 = f\"{up:^15}\"\n",
        "                    c2 = f\"{left:<7}{right:>8}\"\n",
        "                    c3 = f\"{down:^15}\"\n",
        "                line1.append(c1)\n",
        "                line2.append(c2)\n",
        "                line3.append(c3)\n",
        "\n",
        "            # agora cada linha recebe delimitadores\n",
        "            print(\"|\" + \"|\".join(line1) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line2) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line3) + \"|\")\n",
        "        print(horiz)\n"
      ],
      "metadata": {
        "id": "kYja8_jtXYQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q-learning no 4x3\n",
        "\n",
        "```\n",
        "=== Política Aprendida (Q-Learning) ===\n",
        "+------+------+------+------+\n",
        "|  →   |  →   |  →   |  G   |\n",
        "+------+------+------+------+\n",
        "|  ↑   |  ##  |  ↑   |  P   |\n",
        "+------+------+------+------+\n",
        "|  S↑  |  ←   |  ↑   |  ←   |\n",
        "+------+------+------+------+\n",
        "\n",
        "=== Valores de Estado V(s) (Q-Learning) ===\n",
        "+--------+--------+--------+--------+\n",
        "|   0.81 |   0.88 |   0.96 |G(0.00) |\n",
        "+--------+--------+--------+--------+\n",
        "|   0.75 |  ####  |   0.64 |P(0.00) |\n",
        "+--------+--------+--------+--------+\n",
        "|   0.70 |   0.64 |   0.43 |   0.03 |\n",
        "+--------+--------+--------+--------+\n",
        "\n",
        "=== Q-values (Q-Learning) ===\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.73     |    ↑:0.82     |    ↑:0.87     |    ↑:0.00     |\n",
        "|←:0.75   →:0.81|←:0.78   →:0.88|←:0.79   →:0.96|←:0.00   →:0.00|\n",
        "|    ↓:0.73     |    ↓:0.81     |    ↓:0.66     |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.75     |###############|    ↑:0.64     |    ↑:0.00     |\n",
        "|←:0.70   →:0.69|###############|←:0.35  →:-0.61|←:0.00   →:0.00|\n",
        "|    ↓:0.64     |###############|    ↓:0.10     |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.70     |    ↑:0.38     |    ↑:0.43     |    ↑:-0.34    |\n",
        "|←:0.64   →:0.57|←:0.64   →:0.08|←:-0.03 →:-0.08|←:0.03  →:-0.21|\n",
        "|    ↓:0.61     |    ↓:0.29     |    ↓:-0.07    |    ↓:-0.17    |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "```"
      ],
      "metadata": {
        "id": "vVInDKBwRk7G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60aa4394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9948cb61-6dba-4303-c61d-7ed0c50a48c6"
      },
      "source": [
        "# =======================\n",
        "# Execução do Q-Learning no Grid 4x3\n",
        "# =======================\n",
        "\n",
        "env_grid = GridWorld4x3()\n",
        "agent_q_grid = QLearningAgent(env_grid, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
        "\n",
        "print(\"Treinando Q-Learning no Grid 4x3 por 10.000 passos...\")\n",
        "agent_q_grid.train(steps=10000)\n",
        "print(\"Treinamento concluído.\")\n",
        "\n",
        "viz_grid_q = AgentVisualizer(agent_q_grid, env_grid)\n",
        "\n",
        "print(\"\\n=== Política Aprendida (Q-Learning) ===\")\n",
        "viz_grid_q.print_policy()\n",
        "\n",
        "print(\"\\n=== Valores de Estado V(s) (Q-Learning) ===\")\n",
        "viz_grid_q.print_values()\n",
        "\n",
        "print(\"\\n=== Q-values (Q-Learning) ===\")\n",
        "viz_grid_q.print_qvalues()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando Q-Learning no Grid 4x3 por 10.000 passos...\n",
            "Treinamento concluído.\n",
            "\n",
            "=== Política Aprendida (Q-Learning) ===\n",
            "+------+------+------+------+\n",
            "|  →   |  →   |  →   |  G   |\n",
            "+------+------+------+------+\n",
            "|  ↑   |  ##  |  ↑   |  P   |\n",
            "+------+------+------+------+\n",
            "|  S↑  |  ←   |  ←   |  ←   |\n",
            "+------+------+------+------+\n",
            "\n",
            "=== Valores de Estado V(s) (Q-Learning) ===\n",
            "+--------+--------+--------+--------+\n",
            "|   0.81 |   0.88 |   0.96 |G(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.74 |  ####  |   0.54 |P(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.66 |   0.59 |   0.42 |   0.43 |\n",
            "+--------+--------+--------+--------+\n",
            "\n",
            "=== Q-values (Q-Learning) ===\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.64     |    ↑:0.77     |    ↑:0.81     |    ↑:0.00     |\n",
            "|←:0.68   →:0.81|←:0.65   →:0.88|←:0.76   →:0.96|←:0.00   →:0.00|\n",
            "|    ↓:0.66     |    ↓:0.74     |    ↓:0.59     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.74     |###############|    ↑:0.54     |    ↑:0.00     |\n",
            "|←:0.69   →:0.62|###############|←:0.31  →:-0.85|←:0.00   →:0.00|\n",
            "|    ↓:0.63     |###############|    ↓:0.24     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.66     |    ↑:0.42     |    ↑:0.39     |    ↑:-0.13    |\n",
            "|←:0.59   →:0.53|←:0.59   →:0.42|←:0.42   →:0.34|←:0.43   →:0.02|\n",
            "|    ↓:0.60     |    ↓:0.42     |    ↓:0.40     |    ↓:0.03     |\n",
            "+---------------+---------------+---------------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sarsa no 4x3"
      ],
      "metadata": {
        "id": "KkFE9sonR2m6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "644dba50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338cc842-d593-4a95-f8cb-9aba919604fc"
      },
      "source": [
        "# =======================\n",
        "# Execução do SARSA no Grid 4x3\n",
        "# =======================\n",
        "\n",
        "env_grid = GridWorld4x3()\n",
        "agent_sarsa_grid = SarsaAgent(env_grid, alpha=0.1, gamma=0.99, epsilon=0.1)\n",
        "\n",
        "print(\"Treinando SARSA no Grid 4x3 por 10.000 passos...\")\n",
        "agent_sarsa_grid.train(steps=10000)\n",
        "print(\"Treinamento concluído.\")\n",
        "\n",
        "viz_grid_sarsa = AgentVisualizer(agent_sarsa_grid, env_grid)\n",
        "\n",
        "print(\"\\n=== Política Aprendida (SARSA) ===\")\n",
        "viz_grid_sarsa.print_policy()\n",
        "\n",
        "print(\"\\n=== Valores de Estado V(s) (SARSA) ===\")\n",
        "viz_grid_sarsa.print_values()\n",
        "\n",
        "print(\"\\n=== Q-values (SARSA) ===\")\n",
        "viz_grid_sarsa.print_qvalues()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando SARSA no Grid 4x3 por 10.000 passos...\n",
            "Treinamento concluído.\n",
            "\n",
            "=== Política Aprendida (SARSA) ===\n",
            "+------+------+------+------+\n",
            "|  →   |  →   |  →   |  G   |\n",
            "+------+------+------+------+\n",
            "|  ↑   |  ##  |  ↑   |  P   |\n",
            "+------+------+------+------+\n",
            "|  S↑  |  ←   |  ←   |  ←   |\n",
            "+------+------+------+------+\n",
            "\n",
            "=== Valores de Estado V(s) (SARSA) ===\n",
            "+--------+--------+--------+--------+\n",
            "|   0.82 |   0.89 |   0.97 |G(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.74 |  ####  |   0.78 |P(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.65 |   0.53 |   0.36 |   0.19 |\n",
            "+--------+--------+--------+--------+\n",
            "\n",
            "=== Q-values (SARSA) ===\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.69     |    ↑:0.79     |    ↑:0.86     |    ↑:0.00     |\n",
            "|←:0.65   →:0.82|←:0.67   →:0.89|←:0.79   →:0.97|←:0.00   →:0.00|\n",
            "|    ↓:0.64     |    ↓:0.69     |    ↓:0.55     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.74     |###############|    ↑:0.78     |    ↑:0.00     |\n",
            "|←:0.55   →:0.61|###############|←:0.32  →:-0.69|←:0.00   →:0.00|\n",
            "|    ↓:0.53     |###############|    ↓:0.09     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.65     |    ↑:0.32     |    ↑:0.21     |    ↑:-0.31    |\n",
            "|←:0.53   →:0.44|←:0.53   →:0.22|←:0.36   →:0.10|←:0.19  →:-0.04|\n",
            "|    ↓:0.52     |    ↓:0.26     |    ↓:0.21     |    ↓:-0.01    |\n",
            "+---------------+---------------+---------------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b03a747d"
      },
      "source": [
        "## Ambiente CliffWalking\n",
        "\n",
        "Este ambiente é um gridworld clássico utilizado para ilustrar as diferenças entre algoritmos on-policy e off-policy, como SARSA e Q-Learning.\n",
        "\n",
        "**Descrição:**\n",
        "\n",
        "*   É um grid 4x12.\n",
        "*   O agente começa no canto inferior esquerdo (S - Start).\n",
        "*   O objetivo é alcançar o canto inferior direito (G - Goal).\n",
        "*   A área entre o início e o objetivo na linha inferior é um \"penhasco\" (Cliff).\n",
        "*   Cair do penhasco resulta em uma grande penalidade (-100) e o agente retorna ao estado inicial (S).\n",
        "*   Qualquer outro movimento resulta em uma pequena penalidade (-1).\n",
        "*   O episódio termina ao alcançar o objetivo ou cair do penhasco.\n",
        "\n",
        "Este ambiente destaca a exploração vs. segurança: um agente on-policy (como SARSA) que aprende e segue a mesma política exploratória tende a encontrar um caminho mais seguro longe do penhasco para minimizar o risco durante o treinamento. Um agente off-policy (como Q-Learning) que aprende a política ótima independentemente da política de exploração pode encontrar um caminho \"ótimo\" mais arriscado (próximo ao penhasco) se as transições diretas para o objetivo tiverem valores Q altos, mesmo que a política exploratória frequentemente o leve ao penhasco."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "69w9AngaRhWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código do CliffWalkingEnv"
      ],
      "metadata": {
        "id": "kIkgc7FzPOqm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e22eaee7"
      },
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "\n",
        "class CliffWalkingEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"ansi\"]}\n",
        "\n",
        "    def __init__(self, render_mode: Optional[str] = None):\n",
        "        super().__init__()\n",
        "        self.nrows = 4\n",
        "        self.ncols = 12\n",
        "        self.observation_space = spaces.Discrete(self.nrows * self.ncols)\n",
        "        self.action_space = spaces.Discrete(4)  # 0: up, 1: right, 2: down, 3: left\n",
        "\n",
        "        self.start_pos = (0, 0)\n",
        "        self.goal_pos = (11, 0)\n",
        "        self.cliff_rows = [0]\n",
        "        self.cliff_cols = list(range(1, 11))\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.agent_pos = self.start_pos\n",
        "\n",
        "        # Define movements: up, right, down, left\n",
        "        self.moves = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "\n",
        "    def pos_to_state(self, pos):\n",
        "        x, y = pos\n",
        "        return y * self.ncols + x\n",
        "\n",
        "    def state_to_pos(self, s):\n",
        "        return (s % self.ncols, s // self.ncols)\n",
        "\n",
        "    def _is_cliff(self, pos):\n",
        "        x, y = pos\n",
        "        return y in self.cliff_rows and x in self.cliff_cols\n",
        "\n",
        "    def _is_goal(self, pos):\n",
        "        return pos == self.goal_pos\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = self.start_pos\n",
        "        return self.pos_to_state(self.agent_pos), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_pos\n",
        "        dx, dy = self.moves[action]\n",
        "        new_pos = (x + dx, y + dy)\n",
        "\n",
        "        # Check for boundary conditions\n",
        "        if not (0 <= new_pos[0] < self.ncols and 0 <= new_pos[1] < self.nrows):\n",
        "            new_pos = self.agent_pos # Stay in place if hit a wall\n",
        "\n",
        "        self.agent_pos = new_pos\n",
        "\n",
        "        reward = -1 # default reward\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        info = {}\n",
        "\n",
        "        if self._is_cliff(self.agent_pos):\n",
        "            reward = -100\n",
        "            terminated = True\n",
        "            self.agent_pos = self.start_pos # Return to start after falling\n",
        "        elif self._is_goal(self.agent_pos):\n",
        "            reward = 0\n",
        "            terminated = True\n",
        "\n",
        "\n",
        "        return self.pos_to_state(self.agent_pos), reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode=\"ansi\"):\n",
        "        if mode == \"ansi\":\n",
        "            return self._render_ansi()\n",
        "        else:\n",
        "            super().render(mode=mode)\n",
        "\n",
        "    def _render_ansi(self):\n",
        "        output = \"\"\n",
        "        for r in range(self.nrows - 1, -1, -1): # Iterate from top row down\n",
        "            output += \"+\" + \"---+\" * self.ncols + \"\\n\"\n",
        "            row_str = \"|\"\n",
        "            for c in range(self.ncols):\n",
        "                pos = (c, r)\n",
        "                if pos == self.agent_pos:\n",
        "                    row_str += \" A |\"\n",
        "                elif self._is_cliff(pos):\n",
        "                    row_str += \" C |\"\n",
        "                elif self._is_goal(pos):\n",
        "                    row_str += \" G |\"\n",
        "                elif pos == self.start_pos:\n",
        "                     row_str += \" S |\"\n",
        "                else:\n",
        "                    row_str += \"   |\"\n",
        "            output += row_str + \"\\n\"\n",
        "        output += \"+\" + \"---+\" * self.ncols + \"\\n\"\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizador pro CliffWalkingEnv\n",
        "\n",
        "É bem parecido com o do Grid 4x3, apenas com as dimensões adaptadas"
      ],
      "metadata": {
        "id": "e3SUsUHdOovr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CliffAgentVisualizer:\n",
        "    def __init__(self, agent, env):\n",
        "        \"\"\"\n",
        "        agent: agente com V(s), Q(s,a) e greedy_action(s)\n",
        "        env: CliffWalkingEnv-like (tem nrows, ncols, pos_to_state, state_to_pos, start_pos, goal_pos, _is_cliff)\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.action_to_str = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\"}\n",
        "\n",
        "        # Precompute special states\n",
        "        self.start_s = self.env.pos_to_state(self.env.start_pos)\n",
        "        self.goal_s = self.env.pos_to_state(self.env.goal_pos)\n",
        "        self.cliff_states = [self.env.pos_to_state((x, y))\n",
        "                             for y in self.env.cliff_rows\n",
        "                             for x in self.env.cliff_cols]\n",
        "\n",
        "    # -----------------------\n",
        "    # Política (setas)\n",
        "    # -----------------------\n",
        "    def print_policy(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s in self.cliff_states:\n",
        "                    content = \" C \"\n",
        "                elif s == self.goal_s:\n",
        "                    content = \" G \"\n",
        "                else:\n",
        "                    a = self.agent.greedy_action(s)\n",
        "                    arrow = self.action_to_str.get(a, \"?\")\n",
        "                    if s == self.start_s:\n",
        "                        content = f\"S{arrow}\"\n",
        "                    else:\n",
        "                        content = arrow\n",
        "                cells.append(f\"{content:^6}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Valores V(s)\n",
        "    # -----------------------\n",
        "    def print_values(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"--------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s in self.cliff_states:\n",
        "                    content = f\"C(---)\"\n",
        "                else:\n",
        "                    v = self.agent.V(s)\n",
        "                    if s == self.goal_s:\n",
        "                        content = f\"G({v:.2f})\"\n",
        "                    elif s == self.start_s:\n",
        "                        content = f\"S({v:.1f})\"\n",
        "                    else:\n",
        "                        content = f\"{v:6.2f}\"\n",
        "                cells.append(f\"{content:^8}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Q-values\n",
        "    # -----------------------\n",
        "    def print_qvalues(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"---------------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            # três linhas por célula\n",
        "            line1, line2, line3 = [], [], []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s in self.cliff_states:\n",
        "                    c1 = \" \" * 15\n",
        "                    c2 = \"#### CLIFF ####\"\n",
        "                    c3 = \" \" * 15\n",
        "                else:\n",
        "                    qvals = [self.agent.Q(s, a) for a in range(4)]\n",
        "                    up = f\"↑:{qvals[0]:.2f}\"\n",
        "                    right = f\"→:{qvals[1]:.1f}\"\n",
        "                    down = f\"↓:{qvals[2]:.2f}\"\n",
        "                    left = f\"←:{qvals[3]:.1f}\"\n",
        "                    c1 = f\"{up:^15}\"\n",
        "                    c2 = f\"{left:<7}{right:>8}\"\n",
        "                    c3 = f\"{down:^15}\"\n",
        "                line1.append(c1)\n",
        "                line2.append(c2)\n",
        "                line3.append(c3)\n",
        "\n",
        "            print(\"|\" + \"|\".join(line1) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line2) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line3) + \"|\")\n",
        "        print(horiz)\n"
      ],
      "metadata": {
        "id": "9xt0thcluPz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execução do Q-learning no CliffWalking\n",
        "\n",
        "Executar o código a seguir deve gerar saídas parecidas como a abaixo (os valores não precisam ser idênticos, mas a política gerada deve passar na beira do penhasco, a partir do estado inicial).\n",
        "\n",
        "```\n",
        "=== Política Aprendida ===\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "|  →   |  ↓   |  ↓   |  ↓   |  →   |  ↓   |  ↓   |  →   |  →   |  →   |  ↓   |  ↓   |\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "|  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  ↓   |\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "|  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  ↓   |\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "|  S↑  |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  G   |\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "\n",
        "=== Valores de Estado V(s) ===\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "| -12.50 | -11.82 | -10.99 | -10.00 |  -9.00 |  -8.00 |  -7.00 |  -6.00 |  -5.00 |  -4.00 |  -3.00 |  -2.00 |\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "| -12.00 | -11.00 | -10.00 |  -9.00 |  -8.00 |  -7.00 |  -6.00 |  -5.00 |  -4.00 |  -3.00 |  -2.00 |  -1.00 |\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "| -11.00 | -10.00 |  -9.00 |  -8.00 |  -7.00 |  -6.00 |  -5.00 |  -4.00 |  -3.00 |  -2.00 |  -1.00 |   0.00 |\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "|S(-12.0)| C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) |G(0.00) |\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "\n",
        "=== Q-values ===\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "|   ↑:-12.57    |   ↑:-12.27    |   ↑:-11.68    |   ↑:-10.00    |    ↑:-9.34    |    ↑:-8.60    |    ↑:-7.00    |    ↑:-6.75    |    ↑:-5.91    |    ↑:-4.95    |    ↑:-3.81    |    ↑:-2.87    |\n",
        "|←:-12.6 →:-12.5|←:-11.8 →:-11.8|←:-11.4 →:-11.0|←:-11.0 →:-10.0|←:-10.5  →:-9.0|←:-8.5   →:-8.0|←:-8.7   →:-7.0|←:-7.8   →:-6.0|←:-6.9   →:-5.0|←:-5.8   →:-4.0|←:-4.9   →:-3.0|←:-3.8   →:-2.9|\n",
        "|   ↓:-12.58    |   ↓:-11.82    |   ↓:-10.99    |   ↓:-10.00    |    ↓:-9.00    |    ↓:-8.00    |    ↓:-7.00    |    ↓:-6.00    |    ↓:-5.00    |    ↓:-4.00    |    ↓:-3.00    |    ↓:-2.00    |\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "|   ↑:-13.19    |   ↑:-12.56    |   ↑:-11.95    |   ↑:-11.00    |   ↑:-10.00    |    ↑:-9.00    |    ↑:-8.00    |    ↑:-7.00    |    ↑:-6.00    |    ↑:-5.00    |    ↑:-4.00    |    ↑:-3.00    |\n",
        "|←:-13.0 →:-12.0|←:-13.0 →:-11.0|←:-12.0 →:-10.0|←:-11.0  →:-9.0|←:-10.0  →:-8.0|←:-9.0   →:-7.0|←:-8.0   →:-6.0|←:-7.0   →:-5.0|←:-6.0   →:-4.0|←:-5.0   →:-3.0|←:-4.0   →:-2.0|←:-3.0   →:-2.0|\n",
        "|   ↓:-12.00    |   ↓:-11.00    |   ↓:-10.00    |    ↓:-9.00    |    ↓:-8.00    |    ↓:-7.00    |    ↓:-6.00    |    ↓:-5.00    |    ↓:-4.00    |    ↓:-3.00    |    ↓:-2.00    |    ↓:-1.00    |\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "|   ↑:-13.00    |   ↑:-12.00    |   ↑:-11.00    |   ↑:-10.00    |    ↑:-9.00    |    ↑:-8.00    |    ↑:-7.00    |    ↑:-6.00    |    ↑:-5.00    |    ↑:-4.00    |    ↑:-3.00    |    ↑:-2.00    |\n",
        "|←:-12.0 →:-11.0|←:-12.0 →:-10.0|←:-11.0  →:-9.0|←:-10.0  →:-8.0|←:-9.0   →:-7.0|←:-8.0   →:-6.0|←:-7.0   →:-5.0|←:-6.0   →:-4.0|←:-5.0   →:-3.0|←:-4.0   →:-2.0|←:-3.0   →:-1.0|←:-2.0   →:-1.0|\n",
        "|   ↓:-13.00    |   ↓:-100.00   |   ↓:-100.00   |   ↓:-100.00   |   ↓:-100.00   |   ↓:-100.00   |   ↓:-100.00   |   ↓:-100.00   |   ↓:-100.00   |   ↓:-100.00   |   ↓:-100.00   |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "|   ↑:-12.00    |               |               |               |               |               |               |               |               |               |               |    ↑:0.00     |\n",
        "|←:-13.0→:-100.0|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|←:0.0     →:0.0|\n",
        "|   ↓:-13.00    |               |               |               |               |               |               |               |               |               |               |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "```"
      ],
      "metadata": {
        "id": "wx5vkCVrPeMv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4908175e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76229437-24a2-4763-f316-0b4a481f5e0e"
      },
      "source": [
        "# =======================\n",
        "# Exemplo de uso\n",
        "# =======================\n",
        "\n",
        "env = CliffWalkingEnv()\n",
        "agent = QLearningAgent(env, alpha=0.5, gamma=1.0, epsilon=0.1)\n",
        "\n",
        "agent.train(steps=100_000)\n",
        "\n",
        "viz = CliffAgentVisualizer(agent, env)\n",
        "\n",
        "print(\"\\n=== Política Aprendida ===\")\n",
        "viz.print_policy()\n",
        "\n",
        "print(\"\\n=== Valores de Estado V(s) ===\")\n",
        "viz.print_values()\n",
        "\n",
        "print(\"\\n=== Q-values ===\")\n",
        "viz.print_qvalues()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Política Aprendida ===\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "|  ←   |  →   |  ↓   |  →   |  ↓   |  →   |  ↓   |  →   |  →   |  →   |  ↓   |  ↓   |\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "|  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  ↓   |\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "|  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  ↓   |\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "|  S↑  |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  G   |\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "\n",
            "=== Valores de Estado V(s) ===\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| -12.27 | -11.78 | -10.98 | -10.00 |  -9.00 |  -8.00 |  -7.00 |  -6.00 |  -5.00 |  -4.00 |  -3.00 |  -2.00 |\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| -12.00 | -11.00 | -10.00 |  -9.00 |  -8.00 |  -7.00 |  -6.00 |  -5.00 |  -4.00 |  -3.00 |  -2.00 |  -1.00 |\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| -11.00 | -10.00 |  -9.00 |  -8.00 |  -7.00 |  -6.00 |  -5.00 |  -4.00 |  -3.00 |  -2.00 |  -1.00 |   0.00 |\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "|S(-12.0)| C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) |G(0.00) |\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "\n",
            "=== Q-values ===\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
            "|   ↑:-12.49    |   ↑:-11.84    |   ↑:-11.05    |   ↑:-10.49    |    ↑:-9.75    |    ↑:-8.66    |    ↑:-7.62    |    ↑:-6.73    |    ↑:-5.33    |    ↑:-4.48    |    ↑:-3.64    |    ↑:-2.50    |\n",
            "|←:-12.3 →:-12.3|←:-12.3 →:-11.8|←:-11.5 →:-11.0|←:-10.8 →:-10.0|←:-10.5  →:-9.0|←:-9.0   →:-8.0|←:-8.5   →:-7.0|←:-7.1   →:-6.0|←:-6.3   →:-5.0|←:-5.9   →:-4.0|←:-4.9   →:-3.0|←:-2.5   →:-2.9|\n",
            "|   ↓:-12.54    |   ↓:-11.87    |   ↓:-10.98    |   ↓:-10.00    |    ↓:-9.00    |    ↓:-8.00    |    ↓:-7.00    |    ↓:-6.00    |    ↓:-5.00    |    ↓:-4.00    |    ↓:-3.00    |    ↓:-2.00    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
            "|   ↑:-12.89    |   ↑:-12.48    |   ↑:-11.93    |   ↑:-11.00    |   ↑:-10.00    |    ↑:-9.00    |    ↑:-8.00    |    ↑:-7.00    |    ↑:-6.00    |    ↑:-5.00    |    ↑:-4.00    |    ↑:-3.00    |\n",
            "|←:-12.3 →:-12.0|←:-13.0 →:-11.0|←:-12.0 →:-10.0|←:-11.0  →:-9.0|←:-10.0  →:-8.0|←:-9.0   →:-7.0|←:-8.0   →:-6.0|←:-7.0   →:-5.0|←:-6.0   →:-4.0|←:-5.0   →:-3.0|←:-4.0   →:-2.0|←:-3.0   →:-2.0|\n",
            "|   ↓:-12.00    |   ↓:-11.00    |   ↓:-10.00    |    ↓:-9.00    |    ↓:-8.00    |    ↓:-7.00    |    ↓:-6.00    |    ↓:-5.00    |    ↓:-4.00    |    ↓:-3.00    |    ↓:-2.00    |    ↓:-1.00    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
            "|   ↑:-13.00    |   ↑:-12.00    |   ↑:-11.00    |   ↑:-10.00    |    ↑:-9.00    |    ↑:-8.00    |    ↑:-7.00    |    ↑:-6.00    |    ↑:-5.00    |    ↑:-4.00    |    ↑:-3.00    |    ↑:-2.00    |\n",
            "|←:-12.0 →:-11.0|←:-12.0 →:-10.0|←:-11.0  →:-9.0|←:-10.0  →:-8.0|←:-9.0   →:-7.0|←:-8.0   →:-6.0|←:-7.0   →:-5.0|←:-6.0   →:-4.0|←:-5.0   →:-3.0|←:-4.0   →:-2.0|←:-3.0   →:-1.0|←:-2.0   →:-1.0|\n",
            "|   ↓:-13.00    |   ↓:-112.00   |   ↓:-112.00   |   ↓:-112.00   |   ↓:-112.00   |   ↓:-112.00   |   ↓:-112.00   |   ↓:-112.00   |   ↓:-112.00   |   ↓:-112.00   |   ↓:-112.00   |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
            "|   ↑:-12.00    |               |               |               |               |               |               |               |               |               |               |    ↑:0.00     |\n",
            "|←:-13.0→:-112.0|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|←:0.0     →:0.0|\n",
            "|   ↓:-13.00    |               |               |               |               |               |               |               |               |               |               |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execução do Sarsa no CliffWalking\n",
        "\n",
        "Executar o código a seguir deve gerar saídas parecidas como a abaixo. Os valores não precisam ser idênticos, mas a política gerada deve passar longe do penhasco, a partir do estado inicial. A política para os outros estados pode variar, já que eles serão bem menos visitados.\n",
        "\n",
        "```\n",
        "=== Política Aprendida ===\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "|  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  →   |  ↓   |\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "|  ↑   |  ↑   |  ↑   |  ↑   |  ↑   |  ↑   |  ↑   |  →   |  ↑   |  ↑   |  →   |  ↓   |\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "|  ↑   |  ↑   |  ↑   |  ↑   |  ↑   |  ↑   |  ↑   |  →   |  ↑   |  ↑   |  →   |  ↓   |\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "|  S↑  |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  G   |\n",
        "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
        "\n",
        "=== Valores de Estado V(s) ===\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "| -16.05 | -13.79 | -12.34 | -11.76 | -11.41 | -10.80 |  -9.45 |  -7.57 |  -7.37 |  -4.27 |  -3.12 |  -2.02 |\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "| -18.89 | -19.29 | -15.12 | -18.39 | -15.25 | -12.36 | -12.33 |  -8.42 | -11.06 |  -6.35 |  -2.32 |  -1.00 |\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "| -18.55 | -21.70 | -18.64 | -18.93 | -16.93 | -18.94 | -17.63 | -16.77 | -12.81 | -11.04 |  -1.00 |   0.00 |\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "|S(-19.5)| C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) |G(0.00) |\n",
        "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
        "\n",
        "=== Q-values ===\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "|   ↑:-19.52    |   ↑:-19.58    |   ↑:-17.73    |   ↑:-17.72    |   ↑:-16.35    |   ↑:-16.39    |   ↑:-15.75    |   ↑:-13.62    |   ↑:-10.74    |    ↑:-7.44    |    ↑:-6.55    |    ↑:-6.18    |\n",
        "|←:-18.4 →:-16.0|←:-19.0 →:-13.8|←:-17.8 →:-12.3|←:-18.8 →:-11.8|←:-16.7 →:-11.4|←:-17.0 →:-10.8|←:-14.6  →:-9.5|←:-13.6  →:-7.6|←:-13.0  →:-7.4|←:-11.5  →:-4.3|←:-7.7   →:-3.1|←:-6.0   →:-4.9|\n",
        "|   ↓:-20.60    |   ↓:-18.65    |   ↓:-18.46    |   ↓:-18.39    |   ↓:-17.65    |   ↓:-16.39    |   ↓:-14.56    |   ↓:-11.11    |   ↓:-10.87    |    ↓:-7.97    |    ↓:-8.14    |    ↓:-2.02    |\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "|   ↑:-18.89    |   ↑:-19.29    |   ↑:-15.12    |   ↑:-18.39    |   ↑:-15.25    |   ↑:-12.36    |   ↑:-12.33    |   ↑:-14.80    |   ↑:-11.06    |    ↑:-6.35    |    ↑:-6.24    |    ↑:-4.38    |\n",
        "|←:-19.5 →:-19.6|←:-24.0 →:-19.7|←:-21.2 →:-21.9|←:-26.8 →:-36.6|←:-21.1 →:-23.5|←:-17.0 →:-18.5|←:-17.6 →:-13.4|←:-14.9  →:-8.4|←:-17.2 →:-15.5|←:-13.5 →:-15.4|←:-9.5   →:-2.3|←:-3.8   →:-3.8|\n",
        "|   ↓:-21.98    |   ↓:-25.38    |   ↓:-22.27    |   ↓:-33.76    |   ↓:-67.16    |   ↓:-20.31    |   ↓:-17.57    |   ↓:-16.80    |   ↓:-37.44    |   ↓:-13.18    |   ↓:-14.37    |    ↓:-1.00    |\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "|   ↑:-18.55    |   ↑:-21.70    |   ↑:-18.64    |   ↑:-18.93    |   ↑:-16.93    |   ↑:-18.94    |   ↑:-17.63    |   ↑:-16.93    |   ↑:-12.81    |   ↑:-11.04    |   ↑:-13.64    |    ↑:-4.26    |\n",
        "|←:-21.5 →:-27.2|←:-21.9 →:-24.8|←:-31.8 →:-35.6|←:-55.7 →:-31.2|←:-21.6 →:-21.3|←:-51.3 →:-31.3|←:-20.3 →:-27.9|←:-24.3 →:-16.8|←:-16.6 →:-33.7|←:-50.4 →:-51.7|←:-9.3   →:-1.0|←:-2.1   →:-1.0|\n",
        "|   ↓:-23.40    |   ↓:-99.99    |   ↓:-98.44    |   ↓:-96.88    |   ↓:-98.44    |   ↓:-50.00    |   ↓:-87.50    |   ↓:-87.50    |   ↓:-98.44    |   ↓:-99.61    |   ↓:-99.99    |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "|   ↑:-19.50    |               |               |               |               |               |               |               |               |               |               |    ↑:0.00     |\n",
        "|←:-31.9→:-100.0|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|←:0.0     →:0.0|\n",
        "|   ↓:-27.32    |               |               |               |               |               |               |               |               |               |               |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "NEOMF8qGQJoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# Exemplo de uso\n",
        "# =======================\n",
        "\n",
        "env = CliffWalkingEnv()\n",
        "agent = SarsaAgent(env, alpha=0.5, gamma=1.0, epsilon=0.1)\n",
        "\n",
        "agent.train(steps=100_000)\n",
        "\n",
        "viz = CliffAgentVisualizer(agent, env)\n",
        "\n",
        "print(\"\\n=== Política Aprendida ===\")\n",
        "viz.print_policy()\n",
        "\n",
        "print(\"\\n=== Valores de Estado V(s) ===\")\n",
        "viz.print_values()\n",
        "\n",
        "print(\"\\n=== Q-values ===\")\n",
        "viz.print_qvalues()\n"
      ],
      "metadata": {
        "id": "S5Nfcr6Tuuns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a84c58-db6a-4066-93e7-f174800f8279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Política Aprendida ===\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "|  →   |  →   |  ↓   |  ↓   |  →   |  →   |  →   |  →   |  ↓   |  ↓   |  →   |  ↓   |\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "|  ↑   |  ↑   |  →   |  →   |  ↑   |  ↑   |  →   |  ↑   |  →   |  →   |  →   |  ↓   |\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "|  ←   |  ↑   |  ↑   |  →   |  ↑   |  ↑   |  ↑   |  ↑   |  ↑   |  ↑   |  →   |  ↓   |\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "|  S↑  |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  C   |  G   |\n",
            "+------+------+------+------+------+------+------+------+------+------+------+------+\n",
            "\n",
            "=== Valores de Estado V(s) ===\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| -25.08 | -23.86 | -22.44 | -22.51 | -16.29 | -13.35 |  -8.21 |  -6.09 |  -5.09 |  -4.26 |  -5.26 |  -2.88 |\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| -26.47 | -25.20 | -19.91 | -18.19 | -16.81 | -15.34 | -14.21 |  -8.99 |  -4.27 |  -3.40 |  -2.27 |  -1.07 |\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "| -27.36 | -26.15 | -18.64 | -20.09 | -19.40 | -21.89 | -20.56 | -19.55 |  -6.33 |  -4.30 |  -1.00 |   0.00 |\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "|S(-28.1)| C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) | C(---) |G(0.00) |\n",
            "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
            "\n",
            "=== Q-values ===\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
            "|   ↑:-26.42    |   ↑:-25.67    |   ↑:-24.86    |   ↑:-24.48    |   ↑:-18.51    |   ↑:-21.23    |   ↑:-20.92    |   ↑:-20.29    |   ↑:-19.57    |   ↑:-19.60    |    ↑:-6.04    |    ↑:-4.42    |\n",
            "|←:-26.3 →:-25.1|←:-26.2 →:-23.9|←:-24.7 →:-24.5|←:-24.2 →:-24.0|←:-24.3 →:-16.3|←:-22.3 →:-13.4|←:-21.0  →:-8.2|←:-18.3  →:-6.1|←:-18.6 →:-20.2|←:-19.3 →:-19.7|←:-7.3   →:-5.3|←:-5.8   →:-7.3|\n",
            "|   ↓:-26.68    |   ↓:-25.70    |   ↓:-22.44    |   ↓:-22.51    |   ↓:-22.29    |   ↓:-20.89    |   ↓:-19.14    |   ↓:-19.96    |    ↓:-5.09    |    ↓:-4.26    |   ↓:-14.67    |    ↓:-2.88    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
            "|   ↑:-26.47    |   ↑:-25.20    |   ↑:-23.71    |   ↑:-23.68    |   ↑:-16.81    |   ↑:-15.34    |   ↑:-20.82    |    ↑:-8.99    |   ↑:-14.77    |    ↑:-6.15    |    ↑:-5.88    |    ↑:-5.54    |\n",
            "|←:-26.8 →:-26.7|←:-26.3 →:-25.9|←:-25.5 →:-19.9|←:-24.2 →:-18.2|←:-22.7 →:-22.4|←:-23.0 →:-22.1|←:-20.5 →:-14.2|←:-19.7 →:-22.4|←:-12.1  →:-4.3|←:-13.4  →:-3.4|←:-7.6   →:-2.3|←:-5.3   →:-4.5|\n",
            "|   ↓:-28.02    |   ↓:-26.14    |   ↓:-25.37    |   ↓:-22.93    |   ↓:-27.19    |   ↓:-22.42    |   ↓:-20.49    |   ↓:-30.36    |   ↓:-16.40    |    ↓:-6.66    |    ↓:-6.47    |    ↓:-1.07    |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
            "|   ↑:-27.44    |   ↑:-26.15    |   ↑:-18.64    |   ↑:-39.02    |   ↑:-19.40    |   ↑:-21.89    |   ↑:-20.56    |   ↑:-19.55    |    ↑:-6.33    |    ↑:-4.30    |    ↑:-6.93    |    ↑:-4.19    |\n",
            "|←:-27.4 →:-28.0|←:-27.0 →:-33.5|←:-28.7 →:-29.5|←:-38.6 →:-20.1|←:-30.5 →:-35.0|←:-27.2 →:-26.6|←:-71.7 →:-68.3|←:-21.8 →:-24.9|←:-63.8 →:-31.9|←:-28.5 →:-17.5|←:-7.6   →:-1.0|←:-2.1   →:-1.0|\n",
            "|   ↓:-29.59    |   ↓:-124.39   |   ↓:-128.17   |   ↓:-111.41   |   ↓:-78.93    |   ↓:-119.72   |   ↓:-101.30   |   ↓:-120.59   |   ↓:-112.59   |   ↓:-113.63   |   ↓:-120.82   |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
            "|   ↑:-28.12    |               |               |               |               |               |               |               |               |               |               |    ↑:0.00     |\n",
            "|←:-28.5→:-125.7|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|#### CLIFF ####|←:0.0     →:0.0|\n",
            "|   ↓:-28.74    |               |               |               |               |               |               |               |               |               |               |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jiSqcppbuw3h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}