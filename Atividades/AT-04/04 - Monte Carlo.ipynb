{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardofae/RL/blob/main/AT-04/04%20-%20Monte%20Carlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métodos de Monte Carlo\n",
        "\n",
        "Este Colab foi preparado para que você implemente um algoritmo de Monte Carlo em um ambiente Gridworld 4x3.\n",
        "\n",
        "Você irá:\n",
        "\n",
        "1. **Implementar o algoritmo de first-visit Monte Carlo control**: Preencher as partes faltando na classe `MonteCarloAgent` para inicializar o agente, atualizar estimativas a partir de uma trajetória num episódio e treinar o agente, gerando as trajetórias.\n",
        "3. **Visualizar os resultados**: Utilizar as ferramentas de visualização fornecidas para inspecionar os valores dos estados e a política resultante.\n",
        "\n",
        "Ao final, você terá uma implementação funcional de um agente de Monte Carlo e uma compreensão de como ele pode ser aplicado para encontrar a política ótima em um ambiente com dinâmica desconhecida."
      ],
      "metadata": {
        "id": "hSPuBhqcodKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid 4x3\n",
        "\n",
        "O código abaixo deve ser preenchido com a implementação do grid 4x3, com as mesmas especificações do Colab de [MDPs](https://colab.research.google.com/drive/1iNG9EX1-piXQvmN4-wQzncBv4EX7-yZV?usp=sharing). Portanto, você pode/deve aproveitar o código já feito. Não há mais aquelas funções adicionais para permitir que o ambiente seja \"consultado\" para execução de métodos de programação dinâmica, porque agora o agente não vai conhecer a dinâmica do ambiente. Ele irá interagir somente pela API padrão do gymnasium.\n"
      ],
      "metadata": {
        "id": "3lf22cmCHwlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Especificação do GridWorld (igual à tarefa de MDPs)\n",
        "\n",
        "A célula abaixo tem a especificação do grid 4x3, igual no colab de MDPs. Pode pular se você já tiver feito."
      ],
      "metadata": {
        "id": "6L9Xbw9jpArh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Implemente o ambiente do grid 4x3, preenchendo as células abaixo. Você deve permitir ao usuário especificar a recompensa de cada passo (padrão = -0.04), a probabilidade de 'escorregar' (padrão = 0.2) e o numero máximo de passos antes de encerrar o episódio. O espaço de ações deve ser discreto, com 4 ações (0=UP, 1=RIGHT, 2=DOWN, 3=LEFT), e o de estados também será discreto, com 12 estados (mesmo o estado 'parede' pode ser considerado nesta contagem). A convenção para numeração dos estados é (G=goal, #=parede, P=pit/buraco,S=start):\n",
        "```\n",
        "y=2    +----+----+----+----+\n",
        "       |  8 |  9 | 10 | 11G|\n",
        "       +----+----+----+----+\n",
        "y=1    |  4 |  5#|  6 |  7P|\n",
        "       +----+----+----+----+\n",
        "y=0    |  0S|  1 |  2 |  3 |\n",
        "       +----+----+----+----+\n",
        "        x=0   x=1   x=2   x=3\n",
        "```\n",
        "\n",
        "Note que há métodos para converter a numeração de estados para as coordenadas x,y"
      ],
      "metadata": {
        "id": "1T3ssw9boxKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código do GridWorld 4x3"
      ],
      "metadata": {
        "id": "CPRbrlPlq532"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlzbqKU-fTHm"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "from typing import Optional,Iterable,Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class GridWorld4x3(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"ansi\"]}\n",
        "\n",
        "    UP = 0\n",
        "    RIGHT = 1\n",
        "    DOWN = 2\n",
        "    LEFT = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        reward_step: float = -0.04,\n",
        "        slip: float = 0.2,\n",
        "        max_steps: int = 1000,\n",
        "        seed: Optional[int] = None,\n",
        "        render_mode: Optional[str] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        super().__init__()\n",
        "        self.reward_step = reward_step\n",
        "        self.slip = slip\n",
        "        self.seed = seed\n",
        "\n",
        "        # Grid Description\n",
        "        self.grid_size = (4,3)\n",
        "        self.wall  = [5]\n",
        "        self.goal  = [11]\n",
        "        self.pit   = [7]\n",
        "        self.start = 0\n",
        "\n",
        "        # Rewards\n",
        "        self.goal_reward = 1\n",
        "        self.pit_reward = -1\n",
        "\n",
        "        # Agent Position\n",
        "        self.agent_state = self.start\n",
        "\n",
        "        # Definition of action space\n",
        "        self.n_actions = 4\n",
        "        self.action_space = spaces.Discrete(self.n_actions)\n",
        "\n",
        "        # Definition of observation space\n",
        "        self.n_states = self.grid_size[0]*self.grid_size[1]\n",
        "        self.observation_space = spaces.Discrete(self.n_states)\n",
        "\n",
        "        # COISAS PARA O VIZ\n",
        "        self.wall_pos = (1,1)\n",
        "        self.goal_pos = (3,2)\n",
        "        self.pit_pos = (3,1)\n",
        "        self.start_pos = (0,0)\n",
        "        self.nrows = self.grid_size[1]\n",
        "        self.ncols = self.grid_size[0]\n",
        "\n",
        "        # Update Render Mode\n",
        "        if render_mode:\n",
        "            self.metadata = {\"render_modes\": [render_mode]}\n",
        "\n",
        "    # ---------- conversão estado/posição ----------\n",
        "    def pos_to_state(self, pos):\n",
        "        x, y = pos\n",
        "        return y * self.grid_size[0] + x\n",
        "\n",
        "    def state_to_pos(self, s):\n",
        "        return (s % self.grid_size[0], s // self.grid_size[0])\n",
        "\n",
        "    # ---------- API Gym ----------\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Reset agent position and step count\n",
        "        self.agent_state = self.start\n",
        "\n",
        "        # Returns\n",
        "        obs = self.agent_state\n",
        "        info = {}\n",
        "        return obs, info\n",
        "\n",
        "    def is_vertical(self, action: int):\n",
        "        return action == self.UP or action == self.DOWN\n",
        "\n",
        "    def handle_slip(self, action: int):\n",
        "        direction = 1 if np.random.rand() < 0.5 else -1\n",
        "        action = action + direction\n",
        "        return action%4 if action != -1 else 3\n",
        "\n",
        "    def get_next_state(self, state: int, action: int):\n",
        "        x, y = self.state_to_pos(state)\n",
        "\n",
        "        if self.is_vertical(action):\n",
        "            new_y = np.clip(y+(1-action), 0, self.grid_size[1]-1)\n",
        "            if self.pos_to_state((x,new_y)) not in self.wall:\n",
        "                y = new_y\n",
        "        else:\n",
        "            new_x = np.clip(x+(2-action), 0, self.grid_size[0]-1)\n",
        "            if self.pos_to_state((new_x,y)) not in self.wall:\n",
        "                x = new_x\n",
        "\n",
        "        return self.pos_to_state((x, y))\n",
        "\n",
        "    def get_reward(self, state: int):\n",
        "        if state in self.goal:\n",
        "            return self.goal_reward\n",
        "        if state in self.pit:\n",
        "            return self.pit_reward\n",
        "        return self.reward_step\n",
        "\n",
        "    def step(self, action: int):\n",
        "        # Handle slip\n",
        "        if np.random.rand() < 0.2:\n",
        "            action = self.handle_slip(action)\n",
        "\n",
        "        # Update position\n",
        "        self.agent_state = self.get_next_state(self.agent_state, action)\n",
        "\n",
        "        # Checks if goal state reached\n",
        "        terminated = self.agent_state in self.goal or self.agent_state in self.pit\n",
        "\n",
        "        # Calculates the reward\n",
        "        reward = self.get_reward(self.agent_state)\n",
        "\n",
        "        # Not using\n",
        "        info = {}\n",
        "\n",
        "        # Returns the agent state\n",
        "        obs = self.agent_state\n",
        "\n",
        "        # Ends the episode when max steps reached\n",
        "        truncated = False\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self, mode=None):\n",
        "        div = ' ' * 7 + '+----' * self.grid_size[0] + '+'\n",
        "        for i in range(self.grid_size[1]):\n",
        "            y = self.grid_size[1]-i-1\n",
        "            print(div)\n",
        "            print(f'y={y:<5}', end='')\n",
        "            for x in range(self.grid_size[0]):\n",
        "                state = self.pos_to_state((x,y))\n",
        "                state_class = 'G' if state in self.goal else '#' if state in self.wall else 'P' if state in self.pit else 'S' if state == self.start else ''\n",
        "                state_print = (str(state) if self.agent_state != state else 'X') + state_class\n",
        "                print(f'|{state_print:^4}', end='')\n",
        "            print('|')\n",
        "        print(div)\n",
        "        print('        ', end='')\n",
        "        for x in range(self.grid_size[0]):\n",
        "            print(f'x={x:<2} ', end='')\n",
        "\n",
        "    def close(self):\n",
        "      pass # nao precisa implementar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codigo pra testar o Env\n",
        "\n",
        "A célula abaixo faz o teste básico pra verificar que o ambiente respeita a interface gymnasium"
      ],
      "metadata": {
        "id": "awdoIzbcZbsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deve continuar passando nos testes do Gymnasium\n",
        "from gymnasium.utils.env_checker import check_env\n",
        "\n",
        "# Criar uma instância do ambiente\n",
        "env = GridWorld4x3()\n",
        "\n",
        "# This will catch many common issues\n",
        "try:\n",
        "    check_env(env)\n",
        "    print(\"Environment passes all checks!\")\n",
        "except Exception as e:\n",
        "    print(f\"Environment has issues: {e}\")"
      ],
      "metadata": {
        "id": "MNfZO-RQgHU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7687c9be-8239-4bee-81d1-c8991de9e978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment passes all checks!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/utils/env_checker.py:434: UserWarning: \u001b[33mWARN: Not able to test alternative render modes due to the environment not having a spec. Try instantiating the environment through `gymnasium.make`\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5921c89e"
      },
      "source": [
        "## Tarefa: Agente de Monte Carlo\n",
        "\n",
        "Agora você irá implementar o algoritmo de first-visit Monte Carlo control. Você irá trabalhar na classe `MonteCarloAgent` fornecida abaixo. Seu objetivo é preencher as partes faltando para que o agente seja capaz de escolher ações e atualizar estimativas de valor para encontrar a política ótima em um ambiente. No caso deste enunciado será o grid 4x3.\n",
        "\n",
        "Você irá:\n",
        "\n",
        "1.  **Implementar a inicialização (`__init__`)**: Prepare o agente para manter o registro dos valores-Q e os retornos para cada par estado-ação visitado durante sua \"vida\".\n",
        "2.  **Implementar `greedy_action(state)`**: Preencha este método para retornar a ação gulosa para um dado estado, com base nos valores-Q correntes.\n",
        "3.  **Implementar `updateQ(episode)`**: Complete este método para receber um episódio completo e atualizar os valores-Q usando a abordagem de First-Visit Monte Carlo. Esta função não deve resetar o agente. Você deve \"continuar\" a atualização a partir dos valores-Q atuais. Você pode testar esta função independente da `train`, já que esta recebe um episódio pronto.\n",
        "4.  **Implementar `train(n_episodes)`**: Desenvolva este método para executar múltiplos episódios utilizando uma política epsilon-gulosa, coletando os dados de cada episódio e utilizando a função `updateQ` para treinar o agente. Esta função também não deve resetar o agente. Você deve \"continuar\" o treino de onde tiver parado.\n",
        "\n",
        "Ao final, você terá um agente capaz de aprender a política ótima do ambiente Gridworld 4x3 utilizando o algoritmo de Monte Carlo first-visit."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class MonteCarloAgent:\n",
        "    def __init__(self, env: gym.Env, gamma: float = 0.9, epsilon: float = 0.1) -> None:\n",
        "        \"\"\"\n",
        "        Alem das variaveis triviais, inicialize os valores-Q como zero pra todos s,a\n",
        "        e as listas de retorno para cada par estado-ação (ou contagens, se for fazer a\n",
        "        atualização incremental).\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.Q_values = np.zeros((self.env.n_states, self.env.n_actions), dtype=np.float64)\n",
        "        self.sa_count = np.zeros((self.env.n_states, self.env.n_actions), dtype=np.int64)\n",
        "\n",
        "\n",
        "    def updateQ(self, episode: list[tuple[int, int, float]]) -> None:\n",
        "        \"\"\"\n",
        "        Atualiza Q(s,a) usando First-Visit Monte Carlo a partir de um episódio completo.\n",
        "        O episodio é uma lista de tuplas (estado, ação, recompensa), indicando a\n",
        "        trajetória do agente naquele episódio. O estado terminal não estará nesta lista,\n",
        "        pois a última ação e recompensa válidas foi feita/recebida antes dele.\n",
        "\n",
        "        Esta função NÃO deve resetar o agente. Se ela for chamada consecutivamente, as\n",
        "        atualizações devem ser cumulativas.\n",
        "        \"\"\"\n",
        "        G = 0\n",
        "        episode.reverse()\n",
        "        sa_episode = [(state, action) for state, action, _ in episode]\n",
        "        for i, (state, action, reward) in enumerate(episode):\n",
        "            G = self.gamma * G + reward\n",
        "            if (state, action) in sa_episode[i+1:]: continue\n",
        "            self.sa_count[state][action] += 1\n",
        "            self.Q_values[state][action] += (G - self.Q_values[state][action]) / self.sa_count[state][action]\n",
        "\n",
        "    def train(self, n_episodes: int) -> None:\n",
        "        \"\"\"\n",
        "        Executa o treino do agente pelo numero de episodios especificado.\n",
        "        Para cada episodio, reseta o ambiente, segue a politica epsilon-gulosa,\n",
        "        gerando a trajetoria até o estado terminal e atualiza as estimativas de Q\n",
        "\n",
        "        Esta função NÃO deve resetar o agente. Se ela for chamada consecutivamente,\n",
        "        as sessões de treino devem ser cumulativas.\n",
        "        \"\"\"\n",
        "        for i in range(n_episodes):\n",
        "            state, _ = self.env.reset()\n",
        "            episode = []\n",
        "            while True:\n",
        "                if np.random.rand() < self.epsilon:\n",
        "                    action = np.random.randint(self.env.n_actions)\n",
        "                else:\n",
        "                    action = self.greedy_action(state)\n",
        "                new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                episode.append((state, action, reward))\n",
        "                if terminated or truncated: break\n",
        "                state = new_state\n",
        "            self.updateQ(episode)\n",
        "\n",
        "    def V(self, state: int) -> float:\n",
        "        \"\"\"Retorna V(s) = max_a Q(s,a)\"\"\"\n",
        "        return max(self.Q_values[state])\n",
        "\n",
        "    def Q(self, state: int, action: int) -> float:\n",
        "        \"\"\"Retorna o valor-Q de um par estado-ação\"\"\"\n",
        "        return self.Q_values[state][action]\n",
        "\n",
        "    def greedy_action(self, state: int) -> int:\n",
        "        \"\"\"Retorna a ação de maior valor para o estado recebido\"\"\"\n",
        "        best_reward = float('-inf')\n",
        "        for action in range(self.env.n_actions):\n",
        "            reward = self.Q(state, action)\n",
        "            if reward > best_reward:\n",
        "                best_reward = reward\n",
        "                best_action = action\n",
        "        return best_action\n"
      ],
      "metadata": {
        "id": "S1bPltlYvOdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilitário para visualizar o agente\n",
        "\n",
        "O mesmo da tarefa anterior, porém \"enxergando\" um pouco menos dos detalhes internos do env"
      ],
      "metadata": {
        "id": "vDWttyDnXZNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código do utilitário"
      ],
      "metadata": {
        "id": "6eV6pDHSZKDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentVisualizer:\n",
        "    def __init__(self, agent, env):\n",
        "        \"\"\"\n",
        "        agent: objeto que implementa V(s), Q(s,a) e greedy_action(s))\n",
        "        env: GridWorld4x3-like (tem nrows, ncols, pos_to_state, state_to_pos, is_terminal, start_pos, goal_pos, pit_pos, wall_pos)\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.action_to_str = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\"}\n",
        "\n",
        "        # Precompute special states\n",
        "        self.wall_s = self.env.pos_to_state(self.env.wall_pos)\n",
        "        self.start_s = self.env.pos_to_state(self.env.start_pos)\n",
        "        self.goal_s = self.env.pos_to_state(self.env.goal_pos)\n",
        "        self.pit_s = self.env.pos_to_state(self.env.pit_pos)\n",
        "\n",
        "    # -----------------------\n",
        "    # Política (setas)\n",
        "    # -----------------------\n",
        "    def print_policy(self):\n",
        "        rows, cols = 3,4\n",
        "        horiz = \"+\" + \"+\".join([\"------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    content = \"##\"\n",
        "                elif s == self.goal_s:\n",
        "                    content = \" G \"\n",
        "                elif s == self.pit_s:\n",
        "                    content = \" P \"\n",
        "                else:\n",
        "                    a = self.agent.greedy_action(s)\n",
        "                    arrow = self.action_to_str.get(a, \"?\")\n",
        "                    if s == self.start_s:\n",
        "                        content = f\"S{arrow}\"\n",
        "                    else:\n",
        "                        content = arrow\n",
        "                cells.append(f\"{content:^6}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Valores V(s)\n",
        "    # -----------------------\n",
        "    def print_values(self):\n",
        "        rows, cols = 3,4\n",
        "        horiz = \"+\" + \"+\".join([\"--------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    content = \"####\"\n",
        "                else:\n",
        "                    v = self.agent.V(s)\n",
        "                    if s == self.goal_s:\n",
        "                        content = f\"G({v:.2f})\"\n",
        "                    elif s == self.pit_s:\n",
        "                        content = f\"P({v:.2f})\"\n",
        "                    else:\n",
        "                        content = f\"{v:6.2f}\"\n",
        "                cells.append(f\"{content:^8}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Q-values\n",
        "    # -----------------------\n",
        "    def print_qvalues(self):\n",
        "        rows, cols = 3,4\n",
        "        horiz = \"+\" + \"+\".join([\"---------------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            # três linhas por célula\n",
        "            line1, line2, line3 = [], [], []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    c1 = \"###############\"\n",
        "                    c2 = \"###############\"\n",
        "                    c3 = \"###############\"\n",
        "                else:\n",
        "                    qvals = [self.agent.Q(s, a) for a in range(4)]\n",
        "                    best = int(np.argmax(qvals))\n",
        "                    up = f\"↑:{qvals[0]:.2f}\"\n",
        "                    left = f\"←:{qvals[3]:.2f}\"\n",
        "                    right = f\"→:{qvals[1]:.2f}\"\n",
        "                    down = f\"↓:{qvals[2]:.2f}\"\n",
        "                    c1 = f\"{up:^15}\"\n",
        "                    c2 = f\"{left:<7}{right:>8}\"\n",
        "                    c3 = f\"{down:^15}\"\n",
        "                line1.append(c1)\n",
        "                line2.append(c2)\n",
        "                line3.append(c3)\n",
        "\n",
        "            # agora cada linha recebe delimitadores\n",
        "            print(\"|\" + \"|\".join(line1) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line2) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line3) + \"|\")\n",
        "        print(horiz)\n"
      ],
      "metadata": {
        "id": "kYja8_jtXYQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de uso - 1 episódio simples\n",
        "\n",
        "Ao executar o código a seguir, a saída esperada por ter executado as ações (acima, abaixo, acima, acima, direita, direita, direita) com reward_step=-0.1, slip=0 e gamma = 1 é como abaixo.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Teste. Parametros: reward_step=-0.1, slip=0, gamma=1\n",
        "=== Política ===\n",
        "+------+------+------+------+\n",
        "|  →   |  →   |  →   |  G   |\n",
        "+------+------+------+------+\n",
        "|  ↑   |  ##  |  ↑   |  P   |\n",
        "+------+------+------+------+\n",
        "|  S↑  |  ↑   |  ↑   |  ↑   |\n",
        "+------+------+------+------+\n",
        "\n",
        "=== V(s) ===\n",
        "+--------+--------+--------+--------+\n",
        "|   0.80 |   0.90 |   1.00 |G(0.00) |\n",
        "+--------+--------+--------+--------+\n",
        "|   0.70 |  ####  |   0.00 |P(0.00) |\n",
        "+--------+--------+--------+--------+\n",
        "|   0.40 |   0.00 |   0.00 |   0.00 |\n",
        "+--------+--------+--------+--------+\n",
        "\n",
        "=== Q(s,a) ===\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |\n",
        "|←:0.00   →:0.80|←:0.00   →:0.90|←:0.00   →:1.00|←:0.00   →:0.00|\n",
        "|    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.70     |###############|    ↑:0.00     |    ↑:0.00     |\n",
        "|←:0.00   →:0.00|###############|←:0.00   →:0.00|←:0.00   →:0.00|\n",
        "|    ↓:0.50     |###############|    ↓:0.00     |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.40     |    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |\n",
        "|←:0.00   →:0.00|←:0.00   →:0.00|←:0.00   →:0.00|←:0.00   →:0.00|\n",
        "|    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "```"
      ],
      "metadata": {
        "id": "Zx3u2B7UXf93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld4x3(reward_step=-0.1, slip=0) # grid deterministico\n",
        "agent = MonteCarloAgent(env, gamma=1) # agente sem desconto\n",
        "viz = AgentVisualizer(agent, env)\n",
        "\n",
        "#0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
        "\n",
        "# politica simples: vai pra cima (0), baixo (2), sobe tudo(0), depois tudo pra direita (1)\n",
        "simple_episode = [\n",
        "    (0, 0, -0.1), #start, up\n",
        "    (4, 2, -0.1), #2,1 - down\n",
        "    (0, 0, -0.1), #start, up\n",
        "    (4, 0, -0.1), #2,1 - up\n",
        "    (8, 1, -0.1), #3,1 - right\n",
        "    (9, 1, -0.1), #3,2 - right\n",
        "    (10, 1, +1), #3,3 - right (done)\n",
        "]\n",
        "\n",
        "print(f\"\\nTeste. Parametros: reward_step={env.reward_step}, slip={env.slip}, gamma={agent.gamma}\")\n",
        "agent.updateQ(simple_episode)\n",
        "\n",
        "print(\"=== Política ===\")\n",
        "viz.print_policy()\n",
        "\n",
        "print(\"\\n=== V(s) ===\")\n",
        "viz.print_values()\n",
        "\n",
        "print(\"\\n=== Q(s,a) ===\")\n",
        "viz.print_qvalues()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Q9UdNLuWOiwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436108c6-3705-4ec1-8729-e93afc9a2682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Teste. Parametros: reward_step=-0.1, slip=0, gamma=1\n",
            "=== Política ===\n",
            "+------+------+------+------+\n",
            "|  →   |  →   |  →   |  G   |\n",
            "+------+------+------+------+\n",
            "|  ↑   |  ##  |  ↑   |  P   |\n",
            "+------+------+------+------+\n",
            "|  S↑  |  ↑   |  ↑   |  ↑   |\n",
            "+------+------+------+------+\n",
            "\n",
            "=== V(s) ===\n",
            "+--------+--------+--------+--------+\n",
            "|   0.80 |   0.90 |   1.00 |G(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.70 |  ####  |   0.00 |P(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.40 |   0.00 |   0.00 |   0.00 |\n",
            "+--------+--------+--------+--------+\n",
            "\n",
            "=== Q(s,a) ===\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |\n",
            "|←:0.00   →:0.80|←:0.00   →:0.90|←:0.00   →:1.00|←:0.00   →:0.00|\n",
            "|    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.70     |###############|    ↑:0.00     |    ↑:0.00     |\n",
            "|←:0.00   →:0.00|###############|←:0.00   →:0.00|←:0.00   →:0.00|\n",
            "|    ↓:0.50     |###############|    ↓:0.00     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.40     |    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |\n",
            "|←:0.00   →:0.00|←:0.00   →:0.00|←:0.00   →:0.00|←:0.00   →:0.00|\n",
            "|    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo de uso - outro episódio simples\n",
        "\n",
        "Ao executar o código a seguir, a saída esperada por ter executado as ações (acima, abaixo, acima, acima, direita, direita, direita) com reward_step=-0.1, slip=0 e gamma = 0.9 é como abaixo.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Teste. Parametros: reward_step=-0.1, slip=0, gamma=0.9\n",
        "=== Política ===\n",
        "+------+------+------+------+\n",
        "|  →   |  →   |  →   |  G   |\n",
        "+------+------+------+------+\n",
        "|  ↑   |  ##  |  ↑   |  P   |\n",
        "+------+------+------+------+\n",
        "|  S↑  |  ↑   |  ↑   |  ↑   |\n",
        "+------+------+------+------+\n",
        "\n",
        "=== V(s) ===\n",
        "+--------+--------+--------+--------+\n",
        "|   0.62 |   0.80 |   1.00 |G(0.00) |\n",
        "+--------+--------+--------+--------+\n",
        "|   0.46 |  ####  |   0.00 |P(0.00) |\n",
        "+--------+--------+--------+--------+\n",
        "|   0.06 |   0.00 |   0.00 |   0.00 |\n",
        "+--------+--------+--------+--------+\n",
        "\n",
        "=== Q(s,a) ===\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |\n",
        "|←:0.00   →:0.62|←:0.00   →:0.80|←:0.00   →:1.00|←:0.00   →:0.00|\n",
        "|    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.46     |###############|    ↑:0.00     |    ↑:0.00     |\n",
        "|←:0.00   →:0.00|###############|←:0.00   →:0.00|←:0.00   →:0.00|\n",
        "|    ↓:0.18     |###############|    ↓:0.00     |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "|    ↑:0.06     |    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |\n",
        "|←:0.00   →:0.00|←:0.00   →:0.00|←:0.00   →:0.00|←:0.00   →:0.00|\n",
        "|    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |\n",
        "+---------------+---------------+---------------+---------------+\n",
        "```"
      ],
      "metadata": {
        "id": "1JkiWbmyaYZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld4x3(reward_step=-0.1, slip=0) # grid deterministico\n",
        "agent = MonteCarloAgent(env, gamma=0.9) # agente sem desconto\n",
        "viz = AgentVisualizer(agent, env)\n",
        "\n",
        "#0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
        "\n",
        "# politica simples: vai pra cima (0), baixo (2), sobe tudo(0), depois tudo pra direita (1)\n",
        "simple_episode = [\n",
        "    (0, 0, -0.1), #start, up\n",
        "    (4, 2, -0.1), #2,1 - down\n",
        "    (0, 0, -0.1), #start, up\n",
        "    (4, 0, -0.1), #2,1 - up\n",
        "    (8, 1, -0.1), #3,1 - right\n",
        "    (9, 1, -0.1), #3,2 - right\n",
        "    (10, 1, +1), #3,3 - right (done)\n",
        "]\n",
        "\n",
        "print(f\"\\nTeste. Parametros: reward_step={env.reward_step}, slip={env.slip}, gamma={agent.gamma}\")\n",
        "agent.updateQ(simple_episode)\n",
        "\n",
        "print(\"=== Política ===\")\n",
        "viz.print_policy()\n",
        "\n",
        "print(\"\\n=== V(s) ===\")\n",
        "viz.print_values()\n",
        "\n",
        "print(\"\\n=== Q(s,a) ===\")\n",
        "viz.print_qvalues()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "z_Y2nbyTaYZD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f50408-0d66-434b-ba52-79f0ddfaa0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Teste. Parametros: reward_step=-0.1, slip=0, gamma=0.9\n",
            "=== Política ===\n",
            "+------+------+------+------+\n",
            "|  →   |  →   |  →   |  G   |\n",
            "+------+------+------+------+\n",
            "|  ↑   |  ##  |  ↑   |  P   |\n",
            "+------+------+------+------+\n",
            "|  S↑  |  ↑   |  ↑   |  ↑   |\n",
            "+------+------+------+------+\n",
            "\n",
            "=== V(s) ===\n",
            "+--------+--------+--------+--------+\n",
            "|   0.62 |   0.80 |   1.00 |G(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.46 |  ####  |   0.00 |P(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.06 |   0.00 |   0.00 |   0.00 |\n",
            "+--------+--------+--------+--------+\n",
            "\n",
            "=== Q(s,a) ===\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |\n",
            "|←:0.00   →:0.62|←:0.00   →:0.80|←:0.00   →:1.00|←:0.00   →:0.00|\n",
            "|    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.46     |###############|    ↑:0.00     |    ↑:0.00     |\n",
            "|←:0.00   →:0.00|###############|←:0.00   →:0.00|←:0.00   →:0.00|\n",
            "|    ↓:0.18     |###############|    ↓:0.00     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.06     |    ↑:0.00     |    ↑:0.00     |    ↑:0.00     |\n",
            "|←:0.00   →:0.00|←:0.00   →:0.00|←:0.00   →:0.00|←:0.00   →:0.00|\n",
            "|    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treino (1000 episódios) e visualização do agente\n",
        "\n",
        "Com 1000 episódios de treino, o agente deve poder encontrar a trajetória correta a partir do estado inicial. Porém, não se assuste se a ação para alguns estados menos visitados não for encontrada."
      ],
      "metadata": {
        "id": "3j8GMPBwaul8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld4x3() # grid com parâmetros padrão\n",
        "agent = MonteCarloAgent(env, gamma=0.9, epsilon=0.2) # agente com desconto e epsilon-greedy\n",
        "viz = AgentVisualizer(agent, env)\n",
        "\n",
        "print(\"Treinando agente por 1000 episódios...\")\n",
        "agent.train(1000)\n",
        "print(\"Treino concluído.\")\n",
        "\n",
        "print(\"\\n=== Política final ===\")\n",
        "viz.print_policy()\n",
        "\n",
        "print(\"\\n=== V(s) final ===\")\n",
        "viz.print_values()\n",
        "\n",
        "print(\"\\n=== Q(s,a) final ===\")\n",
        "viz.print_qvalues()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "5yuh7GHWatqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b23c37b-9956-4ca8-8b33-0c6995dbcd60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando agente por 1000 episódios...\n",
            "Treino concluído.\n",
            "\n",
            "=== Política final ===\n",
            "+------+------+------+------+\n",
            "|  →   |  →   |  →   |  G   |\n",
            "+------+------+------+------+\n",
            "|  ↑   |  ##  |  ↑   |  P   |\n",
            "+------+------+------+------+\n",
            "|  S↑  |  ←   |  ↑   |  ←   |\n",
            "+------+------+------+------+\n",
            "\n",
            "=== V(s) final ===\n",
            "+--------+--------+--------+--------+\n",
            "|   0.52 |   0.70 |   0.91 |G(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.39 |  ####  |   0.58 |P(0.00) |\n",
            "+--------+--------+--------+--------+\n",
            "|   0.26 |   0.16 |   0.30 |   0.52 |\n",
            "+--------+--------+--------+--------+\n",
            "\n",
            "=== Q(s,a) final ===\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.35     |    ↑:0.54     |    ↑:0.71     |    ↑:0.00     |\n",
            "|←:0.35   →:0.52|←:0.37   →:0.70|←:0.48   →:0.91|←:0.00   →:0.00|\n",
            "|    ↓:0.27     |    ↓:0.56     |    ↓:0.45     |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.39     |###############|    ↑:0.58     |    ↑:0.00     |\n",
            "|←:0.29   →:0.27|###############|←:0.22  →:-0.78|←:0.00   →:0.00|\n",
            "|    ↓:0.19     |###############|    ↓:-0.09    |    ↓:0.00     |\n",
            "+---------------+---------------+---------------+---------------+\n",
            "|    ↑:0.26     |    ↑:0.15     |    ↑:0.30     |    ↑:-0.17    |\n",
            "|←:0.17   →:0.12|←:0.16   →:0.03|←:-0.05 →:-0.61|←:0.52  →:-0.69|\n",
            "|    ↓:0.20     |    ↓:0.08     |    ↓:0.08     |    ↓:-0.26    |\n",
            "+---------------+---------------+---------------+---------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "addaf95a"
      },
      "source": [
        "## Bônus: explore sua implementação\n",
        "\n",
        "Teste diversos parâmetros para verificar seu efeito nos valores e na política. Você pode comparar seus resultados daqui com os da sua implementação de iteração de valor, para ver como o agente de Monte Carlo tem mais \"dificuldade\" de obter a política ótima"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_1aftuckEUxs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}