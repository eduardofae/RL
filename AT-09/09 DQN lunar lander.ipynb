{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardofae/RL/blob/main/AT-09/09%20DQN%20lunar%20lander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hoUv4L-J1dy4"
      },
      "source": [
        "# DQN on Lunar Lander\n",
        "\n",
        "**With content from [Neuromatch Academy](https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ReinforcementLearning/lunar_lander.ipynb)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4bc2ec6"
      },
      "source": [
        "## Assignment:  experimenting with DQN Tuning on Lunar Lander\n",
        "\n",
        "You will see in the video and plots below that the initial performance of the DQN agent with the default hyperparameters is probably poor.\n",
        "\n",
        "Your task is to find a better combination of hyperparameters for the DQN model to improve both the learning speed and stability.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Hyperparameter Exploration:** Experiment with different combinations of hyperparameters (you can wrap the `DQN` model definition in the code cell above to facilitate experimentaton). All hyperparams can be varied, but pay close attention to:\n",
        "    *   **Network Architecture (`net_arch`):** Try varying the number of hidden layers and the number of neurons in each layer.\n",
        "    *   **Learning Rate (`learning_rate`)**\n",
        "    *   **Batch Size (`batch_size`) and Buffer Size (`buffer_size`):** Experiment with different sizes for the training batch and the replay buffer.\n",
        "    *   **Exploration Parameters (`exploration_initial_eps`, `exploration_fraction`, `exploration_final_eps`):** Adjust how the agent explores the environment initially and how quickly it transitions to exploiting learned knowledge.\n",
        "    *   Other parameters like `gamma` and `train_freq` can also be explored.\n",
        "\n",
        "2.  **Evaluation of Trials:** For each hyperparameter combination you try, train the model for **10,000 timesteps** (by setting `total_timesteps=10000` in the `model.learn()` call). Observe the \"ep_rew_mean\" in the training logs to get a sense of the learning speed and stability. You can also run the video and plotting cells after each trial to visualize the performance and the reward curve.\n",
        "\n",
        "3.  **Select Best Configuration:** Based on your trials, identify the hyperparameter combination that shows the best balance of learning speed (reward increasing quickly) and stability (minimal fluctuations in reward).\n",
        "\n",
        "4.  **Train with Best Configuration:** Once you have found your best configuration, run 5 repetitions of training for **10,000 timesteps** and testing a model with these hyperparameters. Plot 'shaded' graphs of loss and reward over the course of training (see an example below and a guide in Item 2 of the guide [here](https://rll.berkeley.edu/deeprlcoursesp17/docs/plotting_handout.pdf). Notice the fluctuations on performance or their absence.<img src='https://learn2learn.net/assets/img/examples/cheetah_fwdbwd_rewards.png' height=\"300\"/>\n",
        "\n",
        "**Submission:**\n",
        "\n",
        "Answer the questions in the moodle quiz, send the required plots and the downloaded .ipynb (or python project, if you did offline)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding TensorBoard"
      ],
      "metadata": {
        "id": "ZkUFaih_KXyf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9661817"
      },
      "source": [
        "\n",
        "\n",
        "This notebook uses TensorBoard for visualization. To generate the plots for submission, you might need to export the data of the tensorboard plots, or create code to parse and generate your own plots. If you're familiar with the use of TensorBoard, skip this cell. Otherwise, read on.\n",
        "\n",
        "TensorBoard is a visualization tool provided with TensorFlow. It allows you to visualize your model's graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through the graph.\n",
        "\n",
        "In the context of training reinforcement learning agents with Stable-Baselines3, TensorBoard is primarily used to visualize various training metrics that are automatically logged, such as:\n",
        "\n",
        "*   **Episode Rewards:** Shows how the agent's performance (measured by the total reward collected per episode) changes over time.\n",
        "*   **Episode Length:** Indicates the duration of each episode in terms of timesteps.\n",
        "*   **Loss:** Shows the value of the loss function during training, which indicates how well the model is predicting the optimal actions.\n",
        "*   **Learning Rate:** Tracks the learning rate schedule if one is used.\n",
        "\n",
        "**How to use the controls:**\n",
        "\n",
        "When you launch TensorBoard, you will see a web interface. Key controls and features include:\n",
        "\n",
        "*   **Scalars:** This is where you'll find the plots for metrics like episode reward, episode length, and loss.\n",
        "*   **Runs:** If you train your model multiple times with different hyperparameters or seeds, each training run will appear here. You can select or deselect runs to compare their performance on the same plot.\n",
        "*   **Smoothing:** On each scalar plot, there is usually a \"Smoothing\" slider. This slider controls how much the raw data is smoothed.\n",
        "    *   **Raw Values (Smoothing at 0):** Shows the exact value of the metric at each logged step. This can appear very noisy, especially in the early stages of training.\n",
        "    *   **Smoothed Values (Smoothing > 0):** Applies a moving average or other smoothing technique to the data. This helps to see the overall trend of the metric, making it easier to assess learning speed and stability. You can adjust the slider to see varying degrees of smoothing.\n",
        "*   **Zoom and Pan:** You can usually zoom in on specific areas of the plots and pan around to examine details.\n",
        "*   **Download Data:** You can often download the raw data for a plot in CSV or JSON format for further analysis.\n",
        "\n",
        "By examining the plots in TensorBoard, particularly the episode rewards and loss with varying levels of smoothing, you can gain insights into your agent's learning process, identify whether it is improving, and assess the stability of the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "UZttlBu21dy5"
      },
      "source": [
        "---\n",
        "# Setup\n",
        "\n",
        "Installs and import packages, then defines a helper function to play videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "fEaAO6KC1dy6"
      },
      "outputs": [],
      "source": [
        "# @title Install required packages\n",
        "!pip install swig --quiet # SWIG is a development tool that connects programs written in C and C++ with a variety of high-level programming languages. It's used by Box2D.\n",
        "!pip install gymnasium[box2d] --quiet # Gymnasium is a fork of OpenAI Gym, providing environments for reinforcement learning. Box2D is a 2D physics engine used in the Lunar Lander environment.\n",
        "!pip install 'stable-baselines3[extra]' --quiet # Stable-Baselines3 is a set of reliable implementations of reinforcement learning algorithms. The '[extra]' includes additional dependencies like rendering.\n",
        "!pip install pyvirtualdisplay --quiet # pyvirtualdisplay is used to create a virtual display, which is necessary for rendering the environment in a Colab environment.\n",
        "!pip install tensorboard --quiet    # allows monitoring the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "ZZ-bKnTC1dy6"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import base64\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gymnasium as gym\n",
        "\n",
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.results_plotter import ts2xy, load_results # utility functions for plotting results: ts2xy converts timesteps and episode rewards into x and y coordinates, and load_results loads the training logs.\n",
        "from stable_baselines3.common.callbacks import EvalCallback # EvalCallback, which is used to evaluate the agent's performance periodically during training and log the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Q-NNXKQT1dy6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Play Video function\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# create the directory to store the video(s)\n",
        "os.makedirs(\"./video\", exist_ok=True)\n",
        "\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment\n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "def render_mp4(videopath: str) -> str:\n",
        "  \"\"\"\n",
        "  Gets a string containing a b4-encoded version of the MP4 video\n",
        "  at the specified path.\n",
        "  \"\"\"\n",
        "  mp4 = open(videopath, 'rb').read()\n",
        "  base64_encoded_mp4 = b64encode(mp4).decode()\n",
        "  return f'<video width=400 controls><source src=\"data:video/mp4;' \\\n",
        "         f'base64,{base64_encoded_mp4}\" type=\"video/mp4\"></video>'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "052d2233"
      },
      "source": [
        "# @title Record Video function\n",
        "def record_and_display_video(env_name, model, video_name, num_episodes=1):\n",
        "    \"\"\"\n",
        "    Records a video of the agent performing in the environment and displays it.\n",
        "\n",
        "    Args:\n",
        "        env_name (str): The name of the environment.\n",
        "        model (stable_baselines3.DQN): The trained model.\n",
        "        video_name (str): The name to use for the video file.\n",
        "        num_episodes (int): The number of episodes to record (default is 1).\n",
        "    \"\"\"\n",
        "    # create the directory to store the video(s)\n",
        "    os.makedirs(\"./video\", exist_ok=True)\n",
        "\n",
        "    # Use a virtual display for rendering\n",
        "    display = Display(visible=False, size=(1400, 900))\n",
        "    _ = display.start()\n",
        "\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    env = gym.wrappers.RecordVideo(\n",
        "        env,\n",
        "        video_folder=\"video\",\n",
        "        name_prefix=f\"{env_name}_{video_name}\",\n",
        "        episode_trigger=lambda episode_id: episode_id < num_episodes\n",
        "    )\n",
        "\n",
        "    observation, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    episode_count = 0\n",
        "\n",
        "    while not done:\n",
        "        action, states = model.predict(observation, deterministic=True)\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            episode_count += 1\n",
        "            if episode_count < num_episodes:\n",
        "                observation, _ = env.reset()\n",
        "                done = False\n",
        "\n",
        "\n",
        "    env.close()\n",
        "    display.stop() # Stop the virtual display\n",
        "\n",
        "    print(f\"\\nTotal reward: {total_reward}\")\n",
        "\n",
        "    # show video\n",
        "    html = render_mp4(f\"video/{env_name}_{video_name}-episode-0.mp4\")\n",
        "    return HTML(html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* * *\n",
        "# Introduction"
      ],
      "metadata": {
        "id": "x4GS8N0dL3-c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dc3ZYt5l1dy7"
      },
      "source": [
        "\n",
        "\n",
        "In a standard RL setting, an agent learns optimal behavior from an environment through a feedback mechanism to maximize a given objective. Many algorithms have been proposed in the RL literature that an agent can apply to learn the optimal behavior. One such popular algorithm is the Deep Q-Network (DQN). This algorithm makes use of deep neural networks to compute optimal actions. In this project, your goal is to understand the effect of the number of neural network layers on the algorithm's performance. The performance of the algorithm can be evaluated through two metrics - Speed and Stability.\n",
        "\n",
        "**Speed:** How fast the algorithm reaches the maximum possible reward.\n",
        "\n",
        "**Stability** In some applications (especially when online learning is involved), along with speed, stability of the algorithm, i.e., minimal fluctuations in performance, is equally important.\n",
        "\n",
        "In this project, you do not have to write the DQN code from scratch. You only have to tune the hyperparameters (neural network size, learning rate, etc), observe the performance, and analyze.\n",
        "\n",
        "The chosen RL task is Lunar Lander. This task consists of the lander and a landing pad marked by two flags. The episode starts with the lander moving downwards due to gravity. The objective is to land safely using different engines available on the lander with zero speed on the landing pad as quickly and fuel efficient as possible. Reward for moving from the top of the screen and landing on landing pad with zero speed is between 100 to 140 points. Each leg ground contact yields a reward of 10 points. Firing main engine leads to a reward of -0.3 points in each frame. Firing the side engine leads to a reward of -0.03 points in each frame. An additional reward of -100 or +100 points is received if the lander crashes or comes to rest respectively which also leads to end of the episode.\n",
        "\n",
        "The input state of the Lunar Lander consists of following components:\n",
        "\n",
        "  1. Horizontal Position\n",
        "  2. Vertical Position\n",
        "  3. Horizontal Velocity\n",
        "  4. Vertical Velocity\n",
        "  5. Angle\n",
        "  6. Angular Velocity\n",
        "  7. Left Leg Contact\n",
        "  8. Right Leg Contact\n",
        "\n",
        "The actions of the agents are:\n",
        "  1. Do Nothing\n",
        "  2. Fire Main Engine\n",
        "  3. Fire Left Engine\n",
        "  4. Fire Right Engine\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/projects/static/lunar_lander.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorBoard\n",
        "\n",
        "It will start empty because no data has been logged. As the training goes, click the refresh button on the tensorboard to load new data."
      ],
      "metadata": {
        "id": "izvrhbaCMY2f"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "509f8e42"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f5f3d3d"
      },
      "source": [
        "# Specify the log directory and launch TensorBoard\n",
        "log_dir = \"/tmp/gym/\" # Make sure this matches the log_dir used in your training code\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* * *\n",
        "# DQN Implementation from Stable Baselines 3"
      ],
      "metadata": {
        "id": "i7w08ToiLs-f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6be8ff9"
      },
      "source": [
        "\n",
        "\n",
        "We will now use the DQN algorithm using Stable-Baselines3, a set of consolidated implementations of reinforcement learning algorithms in PyTorch. It provides a straightforward way to train and evaluate various RL agents, including DQN.\n",
        "\n",
        "The model accepts various hyperparameters, and you'll be playing with some."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "VN8Al7ay1dy7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create environment\n",
        "env_name = 'LunarLander-v3'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# Wrap the environment with a Monitor to log training progress\n",
        "# so we don't need to manually record statistics\n",
        "env = stable_baselines3.common.monitor.Monitor(env, log_dir)\n",
        "\n",
        "# neural network hyperparameters\n",
        "# net_arch is a list of number of neurons per hidden layer, e.g. [16,20] means\n",
        "# two hidden layers with 16 and 20 neurons, respectively\n",
        "policy_kwargs = dict(activation_fn=torch.nn.ReLU,\n",
        "                     net_arch=[8,])\n",
        "\n",
        "# instantiates the model using the defined hyperparameters\n",
        "model = DQN(\"MlpPolicy\",\n",
        "    env,policy_kwargs = policy_kwargs,\n",
        "    learning_rate=0.1 ,\n",
        "    batch_size=1,  # number of samples taken in each gradient descent update\n",
        "    buffer_size=1,  # size (number of experience tupleS) of the replay buffer.\n",
        "    learning_starts=1,  # how many steps to interact with the environment without updates to the model\n",
        "    gamma=0.99,  # discount factor\n",
        "    target_update_interval=1,  # steps between updates of the target network (1= update every step)\n",
        "    train_freq=(1,\"step\"),  # frequency of model updates (1,'step') meaNs train the network at every step\n",
        "    exploration_initial_eps = 1,  # initial value of random action probability\n",
        "    exploration_fraction = 1,  # fraction of entire training period over which epsilon is decreased\n",
        "    exploration_final_eps=0.5,  # final value of random action probability\n",
        "    seed = 1,  # seed for the pseudo random generators\n",
        "    verbose=0, # Set verbose to 1 to observe training logs.\n",
        "    tensorboard_log=log_dir # where to store training info for tensorboard\n",
        ")\n",
        "\n",
        "# You can also experiment with other RL algorithms like A2C, PPO, DDPG etc.\n",
        "# Refer to  https://stable-baselines3.readthedocs.io/en/master/guide/examples.html\n",
        "# for documentation. For example, if you would like to run DDPG, just replace \"DQN\" above with \"DDPG\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "lBJ6_98x1dy8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Showing the shape of observation and #actions\n",
        "print('State shape: ', env.observation_space.shape)\n",
        "print('Number of actions: ', env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a18c7d5e",
        "cellView": "form"
      },
      "source": [
        "# @title Video of one episode of the untrained model\n",
        "record_and_display_video(env_name, model, \"untrained\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "4DJbr9d21dy8"
      },
      "source": [
        "### Training DQN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "hmdfnbpL1dy8"
      },
      "outputs": [],
      "source": [
        "# For evaluating the performance of the agent periodically and logging the results.\n",
        "callback = EvalCallback(env, log_path=log_dir, deterministic=True)\n",
        "\n",
        "#model.learn(total_timesteps=10_000, callback=callback, progress_bar=True)\n",
        "model.learn(total_timesteps=10_000, log_interval=10, callback=callback, progress_bar=True)\n",
        "# The performance of the training will be registered every 'log_interval' episodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "LliuB6BC1dy8"
      },
      "source": [
        "The training takes time. We encourage you to analyze the output logs (set verbose to 1 to print the output logs). The main component of the logs that you should track is \"ep_rew_mean\" (mean of episode rewards). As the training proceeds, the value of \"ep_rew_mean\" should increase. The improvement need not be monotonic, but the trend should be upwards!\n",
        "\n",
        "Along with training, we are also periodically evaluating the performance of the current model during the training."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ct5JU0dULbtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "3NP3r3iG1dy8"
      },
      "source": [
        "Now, let us look at the visual performance of the trained lander.\n",
        "\n",
        "**Note:** The performance varies across different seeds and runs. This code is not optimized to be stable across all runs and seeds. The idea is to find a robust hyperparameter configuration, but performance is expected to vary anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "kM3Wqbwf1dy9"
      },
      "outputs": [],
      "source": [
        "record_and_display_video(env_name, model, \"after-training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8mwPYtaj1dy9"
      },
      "source": [
        "### Performance over time\n",
        "\n",
        "Let us analyze the model's performance (speed and stability). For this purpose, we plot the number of time steps on the x-axis and the episodic reward given by the trained model on the y-axis.\n",
        "\n",
        "An episode is considered successful when it is finished with reward >= 200.\n",
        "\n",
        "Notice that points are not evenly spaced on the graph. Moreover, this output is different from tensorboard because tensorboard sample frequency is smaller (this shows every episode).\n",
        "\n",
        "**Warning**: just re-running the experiment and plotting again will accumulate results (i.e. just reexecuting the train and plot cells will result in 20000 timesteps in the x-axis). You must store each execution in a different place to see the results properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "-t5a8Cmm1dy9"
      },
      "outputs": [],
      "source": [
        "# Load training results from the log directory and convert them into x (timesteps) and y (episode rewards) coordinates for plotting.\n",
        "x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
        "plt.plot(x, y)\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Episode Rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "OjbFm9e81dy9"
      },
      "source": [
        "Probably, both the video and plot showed a poor performance. From the above plot, we observe that, although the maximum reward is achieved quickly. Achieving an episodic reward of > 200 is good. We see that the agent has achieved it in less than 50000 timesteps (speed is good!). However, there are a lot of fluctuations in the performance (stability is not good!).\n",
        "\n",
        "Your objective now is to modify the model hyperparameters and investigate the stability and speed of the chosen configuration.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "fI1gXWXE1dy9"
      },
      "source": [
        "---\n",
        "# Additional Project Ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "XWmHpJ7-1dy_"
      },
      "source": [
        "## Extension to Atari Games\n",
        "\n",
        "In the Lunar Lander task, the input to the algorithm is a vector of state information. Deep RL algorithms can also be applied when the input to the training is image frames, which is the case in the Atari games. For example, consider one of \"DQN-friendly\" Atari game: Pong. In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3). To train the Pong game, you can start with the following sample code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fa487d5"
      },
      "source": [
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "\n",
        "# Create Atari environment.\n",
        "# If you are using Google Colab, you need to install the 'ale-py' package\n",
        "# with 'pip install ale-py==0.7.5'\n",
        "# and also install the pygame package: 'pip install pygame'\n",
        "env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=1, seed=0)\n",
        "\n",
        "# Frame stacking (optional): stack 4 frames to provide the agent with information about the direction of movement.\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Initialize the DQN model with a CNN policy.\n",
        "# n_steps: The number of steps to run for each environment per update\n",
        "model = DQN(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the agent for 10000 timesteps\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"dqn_pong\")\n",
        "\n",
        "# Load the trained model\n",
        "# model = DQN.load(\"dqn_pong\")\n",
        "\n",
        "# Enjoy trained agent\n",
        "# obs = env.reset()\n",
        "# while True:\n",
        "#     action, _states = model.predict(obs, deterministic=True)\n",
        "#     obs, rewards, dones, info = env.step(action)\n",
        "#     env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hPxNb8hF1dzB"
      },
      "source": [
        "---\n",
        "# References\n",
        "\n",
        "1. [Stable Baselines Framework](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html)\n",
        "2. [Lunar Lander Environment](gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZkUFaih_KXyf"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}