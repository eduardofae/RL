{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardofae/RL/blob/main/AT-07/07%20-%20Model%20based%20RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "429a123a"
      },
      "source": [
        "# Tarefa: Implementação e Comparação de Algoritmos de Aprendizado por Reforço Baseado em Modelo\n",
        "\n",
        "Nesta tarefa, vocês irão implementar e comparar três algoritmos de aprendizado por reforço baseados em modelo: Q-learning, Dyna-Q e Prioritized Sweeping. O objetivo é entender como o uso de um modelo do ambiente pode acelerar o aprendizado e como diferentes abordagens de planejamento (como no Dyna-Q e Prioritized Sweeping) afetam a eficiência do aprendizado.\n",
        "\n",
        "Vocês trabalharão com dois ambientes simples: um Gridworld 4x3 e um ambiente de Labirinto (Maze), já fornecidos no notebook.\n",
        "\n",
        "## Parte 1: Implementação dos Agentes\n",
        "\n",
        "Implemente as classes para os seguintes agentes, herdando da classe base `TDAgent` fornecida:\n",
        "\n",
        "1.  **`QLearningAgent`**: Já está parcialmente implementado. Complete a lógica de atualização Q-value.\n",
        "2.  **`DynaQAgent`**: Implemente o agente Dyna-Q, incluindo o aprendizado do modelo e os passos de planejamento.\n",
        "3.  **`PrioritizedSweepingAgent`**: Implemente o agente Prioritized Sweeping, utilizando uma fila de prioridade para focar o planejamento em transições com maior impacto esperado nos valores Q. Dica: use `heapq` do python.\n",
        "\n",
        "Certifiquem-se de que cada agente armazena um modelo da transição real observada no ambiente.\n",
        "\n",
        "## Parte 2: Teste do Modelo (Grid 4x3)\n",
        "\n",
        "Para verificar se o modelo de transição está sendo aprendido corretamente, vocês irão:\n",
        "\n",
        "1.  Executar uma sequência específica de ações que leve o agente do estado inicial a um estado terminal (por exemplo, uma sequência que resulta em vitória).\n",
        "1.  Após a execução desta sequência, inspecionar o modelo aprendido pelos agentes `DynaQAgent` e `PrioritizedSweepingAgent` para garantir que as transições observadas foram corretamente armazenadas. Vocês podem comparar o modelo aprendido com um dicionário esperado contendo as transições dessa sequência.\n",
        "\n",
        "## Parte 3: Comparação de Desempenho (Maze)\n",
        "\n",
        "Compare o desempenho dos agentes implementados no ambiente `MazeEnv`. Para isso:\n",
        "\n",
        "1.  Para cada agente (Q-learning, Dyna-Q e Prioritized Sweeping), execute 30 repetições independentes.\n",
        "2.  Em cada repetição, treine o agente no ambiente Maze e registre o número total de passos necessários para que o agente atinja 10 episódios de sucesso (episódios onde a recompensa final é positiva). Limite o número máximo de passos por repetição a 1.000.000.\n",
        "4.  Visualize a distribuição do número de passos para 10 sucessos para cada agente, utilizando um box plot. Assim é possível ver qual agente é mais robusto.\n",
        "\n",
        "## Análise dos Resultados\n",
        "\n",
        "Com base nos resultados do experimento da Parte 3, analise (não precisa responder):\n",
        "\n",
        "*   Qual algoritmo é mais sample-efficient? Uma observação extra interessante é o tempo de relógio. Qual agente executa mais rápido considerando este novo critério?\n",
        "\n",
        "*   Quais as possíveis vantagens e desvantagens de cada abordagem baseada em modelo neste ambiente específico?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-qQL1MWzlpa"
      },
      "source": [
        "### Grid 4x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evNquQ94zjgh"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "from typing import Optional,Iterable,Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class GridWorld4x3(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"ansi\"]}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        reward_step: float = -0.04,\n",
        "        slip: float = 0.2,\n",
        "        max_steps: int = 1000,\n",
        "        seed: Optional[int] = None,\n",
        "        render_mode: Optional[str] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ncols = 4\n",
        "        self.nrows = 3\n",
        "        self.observation_space = spaces.Discrete(self.ncols * self.nrows)\n",
        "        self.action_space = spaces.Discrete(4)  # 0=up, 1=right, 2=down, 3=left\n",
        "\n",
        "        self.reward_step = reward_step\n",
        "        self.slip = slip\n",
        "        self.max_steps = max_steps\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.start_pos = (0, 0)\n",
        "        self.goal_pos = (3, 2)  # state 11\n",
        "        self.pit_pos = (3, 1)   # state 7\n",
        "        self.wall_pos = (1, 1)  # state 5 (inacessível)\n",
        "\n",
        "        self._rng = np.random.default_rng(seed)\n",
        "        self.steps = 0\n",
        "        self.agent_pos = self.start_pos\n",
        "\n",
        "        # Movimentos: up, right, down, left\n",
        "        self.moves = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "\n",
        "    # ---------- conversão estado/posição ----------\n",
        "    def pos_to_state(self, pos):\n",
        "        x, y = pos\n",
        "        return y * self.ncols + x\n",
        "\n",
        "    def state_to_pos(self, s):\n",
        "        return (s % self.ncols, s // self.ncols)\n",
        "\n",
        "    # ---------- helpers internos ----------\n",
        "    def _move(self, pos, action):\n",
        "        dx, dy = self.moves[action]\n",
        "        x, y = pos\n",
        "        new_pos = (x + dx, y + dy)\n",
        "        # checa limites e parede\n",
        "        if not (0 <= new_pos[0] < self.ncols and 0 <= new_pos[1] < self.nrows):\n",
        "            return pos\n",
        "        if new_pos == self.wall_pos:\n",
        "            return pos\n",
        "        return new_pos\n",
        "\n",
        "    def _reward_and_done(self, pos):\n",
        "        if pos == self.goal_pos:\n",
        "            return 1.0, True\n",
        "        elif pos == self.pit_pos:\n",
        "            return -1.0, True\n",
        "        return self.reward_step, False\n",
        "\n",
        "    # ---------- API Gym ----------\n",
        "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
        "        super().reset(seed=seed)\n",
        "        self.agent_pos = self.start_pos\n",
        "        self.steps = 0\n",
        "        return self.pos_to_state(self.agent_pos), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "\n",
        "        # sorteia se escorrega\n",
        "        if self._rng.random() < self.slip:\n",
        "            if action in [0, 2]:  # up/down → troca por left/right\n",
        "                action = self._rng.choice([1, 3])\n",
        "            else:  # left/right → troca por up/down\n",
        "                action = self._rng.choice([0, 2])\n",
        "\n",
        "        self.agent_pos = self._move(self.agent_pos, action)\n",
        "        reward, terminated = self._reward_and_done(self.agent_pos)\n",
        "        truncated = self.steps >= self.max_steps\n",
        "\n",
        "        return self.pos_to_state(self.agent_pos), reward, terminated, truncated, {}\n",
        "\n",
        "    # ----------------------------\n",
        "    # Rendering\n",
        "    # ----------------------------\n",
        "    def render(self, mode=\"ansi\"):\n",
        "        if mode == \"ansi\":\n",
        "            return self._render_ansi()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def _render_ansi(self):\n",
        "        out = \"\"\n",
        "        for y in reversed(range(self.nrows)):\n",
        "            out += \"+----\" * self.ncols + \"+\\n\"\n",
        "            for x in range(self.ncols):\n",
        "                pos = (x, y)\n",
        "                s = self.pos_to_state(pos)\n",
        "                cell = f\"{s:2d} \"\n",
        "                if pos == self.wall_pos:\n",
        "                    cell = \" ## \"\n",
        "                elif pos == self.goal_pos:\n",
        "                    cell = f\"{s:2d}G\"\n",
        "                elif pos == self.pit_pos:\n",
        "                    cell = f\"{s:2d}P\"\n",
        "                elif pos == self.start_pos:\n",
        "                    cell = f\"{s:2d}S\"\n",
        "                if self.agent_pos == self.state_to_pos(s):\n",
        "                    cell = f\"[{cell.strip()}]\"\n",
        "                out += f\"|{cell:4}\"\n",
        "            out += \"|\\n\"\n",
        "        out += \"+----\" * self.ncols + \"+\\n\"\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E1l6srHhpOa"
      },
      "source": [
        "## Maze\n",
        "\n",
        "Ambiente como o do livro, Fig. 8.2\n",
        "\n",
        "![{4ECCCEA6-BE37-4E9F-96F5-8928DD5CA753}.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTaZ2tXvhoXH"
      },
      "outputs": [],
      "source": [
        "class MazeEnv(gym.Env):\n",
        "    metadata = {\"render.modes\": [\"human\"]}\n",
        "\n",
        "    def __init__(self, step_reward=0.0, max_steps=500):\n",
        "        super().__init__()\n",
        "        self.WORLD_WIDTH = 9\n",
        "        self.WORLD_HEIGHT = 6\n",
        "        self.n_states = self.WORLD_WIDTH * self.WORLD_HEIGHT\n",
        "\n",
        "        # ações: UP, DOWN, LEFT, RIGHT\n",
        "        self.ACTION_UP = 0\n",
        "        self.ACTION_DOWN = 1\n",
        "        self.ACTION_LEFT = 2\n",
        "        self.ACTION_RIGHT = 3\n",
        "        self.actions = [self.ACTION_UP, self.ACTION_DOWN, self.ACTION_LEFT, self.ACTION_RIGHT]\n",
        "\n",
        "        self.action_space = spaces.Discrete(len(self.actions))\n",
        "        self.observation_space = spaces.Discrete(self.n_states)\n",
        "\n",
        "        self.START_STATE = [2, 0]\n",
        "        self.GOAL_STATES = [[0, 8]]\n",
        "        self.obstacles = [[1, 2], [2, 2], [3, 2], [0, 7], [1, 7], [2, 7], [4, 5]]\n",
        "\n",
        "        self.state = None\n",
        "        self.steps = 0\n",
        "        self.max_steps = max_steps\n",
        "        self.step_reward = step_reward\n",
        "\n",
        "    def pos_to_state(self, pos):\n",
        "        x, y = pos\n",
        "        return x * self.WORLD_WIDTH + y\n",
        "\n",
        "    def state_to_pos(self, s):\n",
        "        return divmod(s, self.WORLD_WIDTH)\n",
        "\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.state = self.pos_to_state(self.START_STATE)\n",
        "        self.steps = 0\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state_to_pos(self.state)\n",
        "\n",
        "        if action == self.ACTION_UP:\n",
        "            x = max(x - 1, 0)\n",
        "        elif action == self.ACTION_DOWN:\n",
        "            x = min(x + 1, self.WORLD_HEIGHT - 1)\n",
        "        elif action == self.ACTION_LEFT:\n",
        "            y = max(y - 1, 0)\n",
        "        elif action == self.ACTION_RIGHT:\n",
        "            y = min(y + 1, self.WORLD_WIDTH - 1)\n",
        "\n",
        "        if [x, y] in self.obstacles:\n",
        "            x, y = self.state_to_pos(self.state)\n",
        "\n",
        "        self.state = self.pos_to_state([x, y])\n",
        "        self.steps += 1\n",
        "\n",
        "        if [x, y] in self.GOAL_STATES:\n",
        "            reward = 1.0\n",
        "            terminated = True\n",
        "        else:\n",
        "            reward = self.step_reward\n",
        "            terminated = False\n",
        "\n",
        "        truncated = self.steps >= self.max_steps\n",
        "        return self.state, reward, terminated, truncated, {}\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        grid = np.full((self.WORLD_HEIGHT, self.WORLD_WIDTH), \" \")\n",
        "        for ox, oy in self.obstacles:\n",
        "            grid[ox, oy] = \"#\"\n",
        "        for gx, gy in self.GOAL_STATES:\n",
        "            grid[gx, gy] = \"G\"\n",
        "        x, y = self.state_to_pos(self.state)\n",
        "        grid[x, y] = \"A\"\n",
        "        print(\"\\n\".join(\"\".join(row) for row in grid))\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaIkDdHdxYv8"
      },
      "source": [
        "## Código dos agentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOTXAQWqxWRd"
      },
      "source": [
        "### TDAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIfKwXNTtxFL"
      },
      "outputs": [],
      "source": [
        "from abc import ABC,abstractmethod\n",
        "\n",
        "class TDAgent(ABC):\n",
        "    def __init__(self, env: gym.Env, alpha: float = 0.1, gamma: float = 0.99, epsilon: float = 0.1):\n",
        "        \"\"\"\n",
        "        Construtor do agente TD.\n",
        "\n",
        "        Args:\n",
        "            env: ambiente Gymnasium (ex: gridworld 4x3).\n",
        "            alpha: taxa de aprendizado.\n",
        "            gamma: fator de desconto.\n",
        "            epsilon: taxa de exploração (para política epsilon-greedy).\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        obs_space_size = env.observation_space.n\n",
        "        act_space_size = env.action_space.n\n",
        "        self.q_values = np.zeros((obs_space_size, act_space_size))\n",
        "\n",
        "    def Q(self, state, action) -> float:\n",
        "      \"\"\"Retorna Q(s,a).\"\"\"\n",
        "      return self.q_values[state, action]\n",
        "\n",
        "    def V(self, state) -> float:\n",
        "      \"\"\"Retorna V(s) = max_a Q(s,a).\"\"\"\n",
        "      return np.max(self.q_values[state, :])\n",
        "\n",
        "    def greedy_action(self, state) -> int:\n",
        "      \"\"\"Retorna a ação gulosa (argmax_a Q(s,a)).\"\"\"\n",
        "      return np.argmax(self.q_values[state, :])\n",
        "\n",
        "    def act(self, state) -> int:\n",
        "      \"\"\"Retorna ação epsilon-greedy.\"\"\"\n",
        "      if np.random.rand() < self.epsilon:\n",
        "          return self.env.action_space.sample()\n",
        "      else:\n",
        "          return self.greedy_action(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyT12R1yOG93"
      },
      "source": [
        "### Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VNzIJ4RqRRp"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent(TDAgent):\n",
        "    def updateQ(self, s, a, r, s_next, done: bool):\n",
        "        \"\"\"Atualiza Q(s,a) segundo a regra do Q-Learning.\"\"\"\n",
        "        self.q_values[s,a] += self.alpha * (r + self.gamma * self.V(s_next) - self.Q(s,a))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fVMdAiqyeH2"
      },
      "source": [
        "### Dyna-Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3thWDBflylN7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "class DynaQAgent(TDAgent):\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1, planning_steps=10):\n",
        "        super().__init__(env, alpha, gamma, epsilon)\n",
        "        self.planning_steps = planning_steps\n",
        "        self.model = {}  # (s,a) -> (r, s', done)\n",
        "\n",
        "    def plan(self):\n",
        "        for _ in range(self.planning_steps):\n",
        "            S, A = random.choice(list(self.model.keys()))\n",
        "            R, S_next, Done = self.model[S,A]\n",
        "            self.q_values[S,A] += self.alpha * (R + self.gamma * self.V(S_next) - self.Q(S,A))\n",
        "\n",
        "    def updateQ(self, s, a, r, s_next, done: bool):\n",
        "        self.q_values[s,a] += self.alpha * (r + self.gamma * self.V(s_next) - self.Q(s,a))\n",
        "        self.model[s,a] = (r, s_next, done)\n",
        "        self.plan()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QJrO8_bRip4"
      },
      "source": [
        "### Prioritized Sweeping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZGXXCsURlsn"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "class PrioritizedSweepingAgent(TDAgent):\n",
        "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=0.1, planning_steps=10, theta=1e-4):\n",
        "        super().__init__(env, alpha, gamma, epsilon)\n",
        "        self.planning_steps = planning_steps\n",
        "        self.theta = theta  # threshold for adding to priority queue\n",
        "        self.model = {}  # (s,a) -> (r, s_next, done)\n",
        "        self.preds = {}\n",
        "        self.pq = []\n",
        "\n",
        "    def add_to_queue(self, s, a, r, s_next, done):\n",
        "        priority = abs(r + self.gamma * self.V(s_next) - self.Q(s,a))\n",
        "        if priority >= self.theta:\n",
        "            heapq.heappush(self.pq, (-priority, (s,a)))\n",
        "\n",
        "    def add_prevs(self, s):\n",
        "        if s not in self.preds: return\n",
        "        for S_prev, A_prev in self.preds[s]:\n",
        "            R, _, Done = self.model[S_prev, A_prev]\n",
        "            self.add_to_queue(S_prev, A_prev, R, s, Done)\n",
        "\n",
        "    def plan(self):\n",
        "        for _ in range(self.planning_steps):\n",
        "            if not self.pq: break\n",
        "            _, (S, A) = heapq.heappop(self.pq)\n",
        "            R, S_next, Done = self.model[S,A]\n",
        "            self.q_values[S,A] += self.alpha * (R + self.gamma * self.V(S_next) - self.Q(S,A))\n",
        "            self.add_prevs(S)\n",
        "\n",
        "    def updateQ(self, s, a, r, s_next, done: bool):\n",
        "        self.model[s,a] = (r, s_next, done)\n",
        "        if s_next not in self.preds: self.preds[s_next] = set()\n",
        "        self.preds[s_next].add((s, a))\n",
        "        self.add_to_queue(s, a, r, s_next, done)\n",
        "        self.plan()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA8u4Iesw9Ru"
      },
      "source": [
        "## Visualizador do agente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzw5lnXmzxrD"
      },
      "source": [
        "### Para o Grid 4x3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbJsXXokz0EW"
      },
      "outputs": [],
      "source": [
        "class Grid4x3AgentVisualizer:\n",
        "    def __init__(self, agent, env):\n",
        "        \"\"\"\n",
        "        agent: ValueIterationAgent-like (tem V(s), Q(s,a) e greedy_action(s))\n",
        "        env: GridWorld4x3-like (tem nrows, ncols, pos_to_state, state_to_pos, is_terminal, get_states, start_pos, goal_pos, pit_pos, wall_pos)\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.action_to_str = {0: \"↑\", 1: \"→\", 2: \"↓\", 3: \"←\"}\n",
        "\n",
        "        # Precompute special states\n",
        "        self.wall_s = self.env.pos_to_state(self.env.wall_pos)\n",
        "        self.start_s = self.env.pos_to_state(self.env.start_pos)\n",
        "        self.goal_s = self.env.pos_to_state(self.env.goal_pos)\n",
        "        self.pit_s = self.env.pos_to_state(self.env.pit_pos)\n",
        "\n",
        "    # -----------------------\n",
        "    # Política (setas)\n",
        "    # -----------------------\n",
        "    def print_policy(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    content = \"##\"\n",
        "                elif s == self.goal_s:\n",
        "                    content = \" G \"\n",
        "                elif s == self.pit_s:\n",
        "                    content = \" P \"\n",
        "                else:\n",
        "                    a = self.agent.greedy_action(s)\n",
        "                    arrow = self.action_to_str.get(a, \"?\")\n",
        "                    if s == self.start_s:\n",
        "                        content = f\"S{arrow}\"\n",
        "                    else:\n",
        "                        content = arrow\n",
        "                cells.append(f\"{content:^6}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Valores V(s)\n",
        "    # -----------------------\n",
        "    def print_values(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"--------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    content = \"####\"\n",
        "                else:\n",
        "                    v = self.agent.V(s)\n",
        "                    if s == self.goal_s:\n",
        "                        content = f\"G({v:.2f})\"\n",
        "                    elif s == self.pit_s:\n",
        "                        content = f\"P({v:.2f})\"\n",
        "                    else:\n",
        "                        content = f\"{v:6.2f}\"\n",
        "                cells.append(f\"{content:^8}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Q-values\n",
        "    # -----------------------\n",
        "    def print_qvalues(self):\n",
        "        rows, cols = self.env.nrows, self.env.ncols\n",
        "        horiz = \"+\" + \"+\".join([\"---------------\"] * cols) + \"+\"\n",
        "\n",
        "        for y in reversed(range(rows)):\n",
        "            print(horiz)\n",
        "            # três linhas por célula\n",
        "            line1, line2, line3 = [], [], []\n",
        "            for x in range(cols):\n",
        "                s = self.env.pos_to_state((x, y))\n",
        "                if s == self.wall_s:\n",
        "                    c1 = \"###############\"\n",
        "                    c2 = \"###############\"\n",
        "                    c3 = \"###############\"\n",
        "                else:\n",
        "                    qvals = [self.agent.Q(s, a) for a in range(4)]\n",
        "                    best = int(np.argmax(qvals))\n",
        "                    up = f\"↑:{qvals[0]:.2f}\"\n",
        "                    left = f\"←:{qvals[3]:.2f}\"\n",
        "                    right = f\"→:{qvals[1]:.2f}\"\n",
        "                    down = f\"↓:{qvals[2]:.2f}\"\n",
        "                    c1 = f\"{up:^15}\"\n",
        "                    c2 = f\"{left:<7}{right:>8}\"\n",
        "                    c3 = f\"{down:^15}\"\n",
        "                line1.append(c1)\n",
        "                line2.append(c2)\n",
        "                line3.append(c3)\n",
        "\n",
        "            # agora cada linha recebe delimitadores\n",
        "            print(\"|\" + \"|\".join(line1) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line2) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line3) + \"|\")\n",
        "        print(horiz)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnR1TmjpiJaE"
      },
      "source": [
        "### Para o Maze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8TfKeEdiLz8"
      },
      "outputs": [],
      "source": [
        "class MazeAgentVisualizer:\n",
        "    def __init__(self, agent, env):\n",
        "        \"\"\"\n",
        "        agent: objeto que implementa V(s), Q(s,a), greedy_action(s)\n",
        "        env: MazeEnv (tem WORLD_HEIGHT, WORLD_WIDTH, START_STATE, GOAL_STATES, obstacles)\n",
        "        \"\"\"\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "\n",
        "        # dimensões\n",
        "        self.nrows = self.env.WORLD_HEIGHT\n",
        "        self.ncols = self.env.WORLD_WIDTH\n",
        "\n",
        "        # estados especiais\n",
        "        self.start_s = self.pos_to_state(tuple(self.env.START_STATE))\n",
        "        self.goal_s = [self.pos_to_state(tuple(g)) for g in self.env.GOAL_STATES]\n",
        "        self.wall_s = [self.pos_to_state(tuple(w)) for w in self.env.obstacles]\n",
        "\n",
        "        # mapeamento de ações\n",
        "        self.action_to_str = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\"}\n",
        "\n",
        "    # -----------------------\n",
        "    # Helpers\n",
        "    # -----------------------\n",
        "    def pos_to_state(self, pos):\n",
        "        x, y = pos\n",
        "        return x * self.ncols + y\n",
        "\n",
        "    def state_to_pos(self, s):\n",
        "        return divmod(s, self.ncols)\n",
        "\n",
        "    # -----------------------\n",
        "    # Política (setas)\n",
        "    # -----------------------\n",
        "    def print_policy(self):\n",
        "        horiz = \"+\" + \"+\".join([\"------\"] * self.ncols) + \"+\"\n",
        "\n",
        "        # x=0 topo, x=HEIGHT-1 base\n",
        "        for x in range(self.nrows):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for y in range(self.ncols):\n",
        "                s = self.pos_to_state((x, y))\n",
        "                if s in self.wall_s:\n",
        "                    content = \"##\"\n",
        "                elif s in self.goal_s:\n",
        "                    content = \" G \"\n",
        "                else:\n",
        "                    a = self.agent.greedy_action(s)\n",
        "                    arrow = self.action_to_str.get(a, \"?\")\n",
        "                    content = f\"S{arrow}\" if s == self.start_s else arrow\n",
        "                cells.append(f\"{content:^6}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Valores V(s)\n",
        "    # -----------------------\n",
        "    def print_values(self):\n",
        "        horiz = \"+\" + \"+\".join([\"--------\"] * self.ncols) + \"+\"\n",
        "\n",
        "        for x in range(self.nrows):\n",
        "            print(horiz)\n",
        "            cells = []\n",
        "            for y in range(self.ncols):\n",
        "                s = self.pos_to_state((x, y))\n",
        "                if s in self.wall_s:\n",
        "                    content = \"####\"\n",
        "                else:\n",
        "                    v = self.agent.V(s)\n",
        "                    if s in self.goal_s:\n",
        "                        content = f\"G({v:.2f})\"\n",
        "                    else:\n",
        "                        content = f\"{v:6.2f}\"\n",
        "                cells.append(f\"{content:^8}\")\n",
        "            print(\"|\" + \"|\".join(cells) + \"|\")\n",
        "        print(horiz)\n",
        "\n",
        "    # -----------------------\n",
        "    # Q-values\n",
        "    # -----------------------\n",
        "    def print_qvalues(self):\n",
        "        horiz = \"+\" + \"+\".join([\"---------------\"] * self.ncols) + \"+\"\n",
        "\n",
        "        for x in range(self.nrows):\n",
        "            print(horiz)\n",
        "            line1, line2, line3 = [], [], []\n",
        "            for y in range(self.ncols):\n",
        "                s = self.pos_to_state((x, y))\n",
        "                if s in self.wall_s:\n",
        "                    c1 = c2 = c3 = \"###############\"\n",
        "                else:\n",
        "                    qvals = [self.agent.Q(s, a) for a in range(4)]\n",
        "                    up    = f\"↑:{qvals[0]:.2f}\"\n",
        "                    down  = f\"↓:{qvals[1]:.2f}\"\n",
        "                    left  = f\"←:{qvals[2]:.2f}\"\n",
        "                    right = f\"→:{qvals[3]:.2f}\"\n",
        "                    c1 = f\"{up:^15}\"\n",
        "                    c2 = f\"{left:<7}{right:>8}\"\n",
        "                    c3 = f\"{down:^15}\"\n",
        "                line1.append(c1)\n",
        "                line2.append(c2)\n",
        "                line3.append(c3)\n",
        "            print(\"|\" + \"|\".join(line1) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line2) + \"|\")\n",
        "            print(\"|\" + \"|\".join(line3) + \"|\")\n",
        "        print(horiz)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJEyZhiWxM4t"
      },
      "source": [
        "## Executando os agentes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d41ca718"
      },
      "source": [
        "### Teste 1: Teste do Modelo (Grid 4x3)\n",
        "\n",
        "Este teste verifica se os agentes Dyna-Q e Prioritized Sweeping estão armazenando corretamente as transições do ambiente em seus modelos após a execução de uma sequência de ações pré-definida no ambiente Grid 4x3 sem escorregamento e com recompensa de passo zero.\n",
        "\n",
        "**Sequência de ações:** `simple_episode_win = [0, 0, 1, 1, 1]` (up, up, right, right, right), levando o agente do estado inicial (0,0) ao estado objetivo (3,2).\n",
        "\n",
        "**Resultado esperado:** O modelo aprendido deve conter as seguintes transições observadas:\n",
        "*   `((0, 0), 0)` -> `(0, (0, 1), False)`\n",
        "*   `((0, 1), 0)` -> `(0, (0, 2), False)`\n",
        "*   `((0, 2), 1)` -> `(0, (1, 2), False)`\n",
        "*   `((1, 2), 1)` -> `(0, (2, 2), False)`\n",
        "*   `((2, 2), 1)` -> `(1, (3, 2), True)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkQu9f8-T8pG",
        "outputId": "4a5718d4-9372-40df-9300-612094916ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running simple episode for Dyna-Q agent...\n",
            "Comparing Dyna-Q model with expected:\n",
            "Dyna-Q model matches the expected model.\n",
            "\n",
            "Running simple episode for Prioritized Sweeping agent...\n",
            "Comparing PS model with expected:\n",
            "PS model matches the expected model.\n"
          ]
        }
      ],
      "source": [
        "# Test Dyna-Q and Prioritized Sweeping model storage with a simple episode\n",
        "\n",
        "simple_episode_win = [0, 0, 1, 1, 1] #up, up, right, right, right\n",
        "\n",
        "env_test = GridWorld4x3(slip=0, reward_step=0)\n",
        "\n",
        "# Initialize agents with planning steps set to 0 to only observe model updates from real transitions\n",
        "dyna_q_test_agent = DynaQAgent(env_test, alpha=0.1, gamma=0.99, epsilon=0.1, planning_steps=0)\n",
        "ps_test_agent = PrioritizedSweepingAgent(env_test, alpha=0.1, gamma=0.99, epsilon=0.1, planning_steps=0)\n",
        "\n",
        "# Define the expected model\n",
        "expected_model = { #((x,y), action) -> ((r, next_x, next_y), done)\n",
        "    ((0, 0), 0): (0, (0, 1), False),\n",
        "    ((0, 1), 0): (0, (0, 2), False),\n",
        "    ((0, 2), 1): (0, (1, 2), False),\n",
        "    ((1, 2), 1): (0, (2, 2), False),\n",
        "    ((2, 2), 1): (1, (3, 2), True)\n",
        "}\n",
        "\n",
        "\n",
        "state, _ = env_test.reset()\n",
        "\n",
        "# Run the simple episode for Dyna-Q\n",
        "print(\"Running simple episode for Dyna-Q agent...\")\n",
        "for action in simple_episode_win:\n",
        "    next_state, reward, terminated, truncated, _ = env_test.step(action)\n",
        "    done = terminated or truncated\n",
        "    # Update Q-value and model based on real transition\n",
        "    dyna_q_test_agent.updateQ(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break # Stop if episode finishes\n",
        "\n",
        "# Convert state-action tuples to readable format (pos, action)\n",
        "readable_model_dyna_q = {}\n",
        "for (s, a), (r, s_next, done_sim) in dyna_q_test_agent.model.items():\n",
        "    pos = env_test.state_to_pos(s)\n",
        "    next_pos = env_test.state_to_pos(s_next)\n",
        "    readable_model_dyna_q[(pos, a)] = (r, next_pos, done_sim)\n",
        "\n",
        "# Compare the actual model with the expected model\n",
        "print(\"Comparing Dyna-Q model with expected:\")\n",
        "if readable_model_dyna_q == expected_model:\n",
        "    print(\"Dyna-Q model matches the expected model.\")\n",
        "else:\n",
        "    print(\"Dyna-Q model does NOT match the expected model.\")\n",
        "    print(\"Actual model:\", readable_model_dyna_q)\n",
        "    print(\"Expected model:\", expected_model)\n",
        "\n",
        "\n",
        "# Reset environment and run the simple episode for Prioritized Sweeping\n",
        "print(\"\\nRunning simple episode for Prioritized Sweeping agent...\")\n",
        "state, _ = env_test.reset()\n",
        "for action in simple_episode_win:\n",
        "    next_state, reward, terminated, truncated, _ = env_test.step(action)\n",
        "    done = terminated or truncated\n",
        "    # Update Q-value and model based on real transition\n",
        "    ps_test_agent.updateQ(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    if done:\n",
        "        break # Stop if episode finishes\n",
        "\n",
        "readable_model_ps = {}\n",
        "for (s, a), (r, s_next, done_sim) in ps_test_agent.model.items():\n",
        "    pos = env_test.state_to_pos(s)\n",
        "    next_pos = env_test.state_to_pos(s_next)\n",
        "    readable_model_ps[(pos, a)] = (r, next_pos, done_sim)\n",
        "\n",
        "\n",
        "# Compare the actual model with the expected model\n",
        "print(\"Comparing PS model with expected:\")\n",
        "if readable_model_ps == expected_model:\n",
        "    print(\"PS model matches the expected model.\")\n",
        "else:\n",
        "    print(\"PS model does NOT match the expected model.\")\n",
        "    print(\"Actual model:\", readable_model_ps)\n",
        "    print(\"Expected model:\", expected_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7MHYm3-kPAD"
      },
      "source": [
        "## Comparação no MazeEnv\n",
        "Compare o desempenho dos agentes implementados no ambiente `MazeEnv`. Para isso:\n",
        "\n",
        "1.  Para cada agente (Q-learning, Dyna-Q e Prioritized Sweeping), execute 30 repetições independentes.\n",
        "2.  Em cada repetição, treine o agente no ambiente Maze e registre o número total de passos necessários para que o agente atinja 10 episódios de sucesso (episódios onde a recompensa final é positiva). Limite o número máximo de passos por repetição a 1.000.000.\n",
        "4.  Visualize a distribuição do número de passos para 10 sucessos para cada agente, utilizando um box plot. Assim é possível ver qual agente é mais robusto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1f25a8f0b69f449e8acc20ad203e73b2",
            "bf94eba0b41449029055af044667189a",
            "e709dca5ae034706b813c8b355c03979",
            "386938fd3db6410488d969922300edb7",
            "e3c62854e742451f84239cd7224b03b6",
            "c8c9ca97b83e4b7db30e7e701618d781",
            "1b57e19d26d14fb59326c9326a0ae008",
            "07ae31987671426c973bb7e4df3309b6",
            "78f1e9b86cb94d4ea684c7a77b54a61d",
            "23fd0658c7aa472db04e403774465313",
            "5a1b706782ca410bb95e65d54720aa65"
          ]
        },
        "id": "KiRBF0G58l1D",
        "outputId": "f9825688-24f0-40e3-9fe3-6ca8086a5529"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repetitions:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f25a8f0b69f449e8acc20ad203e73b2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.notebook import tnrange\n",
        "N_REPETITIONS = 30\n",
        "N_STEPS = 1_000_000\n",
        "\n",
        "env = MazeEnv(step_reward=-0.01, max_steps=N_STEPS)\n",
        "steps = {'QL': [], 'Dyna-Q': [], 'PS': []}\n",
        "\n",
        "def train(agent_class, agent_name, n_steps=N_STEPS):\n",
        "    state, _ = env.reset()\n",
        "    agent = agent_class(env)\n",
        "    num_success = 0\n",
        "    for step in range(n_steps):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        succeded = terminated and reward > 0\n",
        "        agent.updateQ(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if succeded:\n",
        "            num_success += 1\n",
        "            if num_success == 10:\n",
        "                steps[agent_name].append(step)\n",
        "                return\n",
        "        if done:\n",
        "            state, _ = env.reset()\n",
        "    steps.append(N_STEPS)\n",
        "\n",
        "for _ in tnrange(N_REPETITIONS, desc='Repetitions'):\n",
        "    train(QLearningAgent, 'QL')\n",
        "    train(DynaQAgent, 'Dyna-Q')\n",
        "    train(PrioritizedSweepingAgent, 'PS')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nAHBK8DLUf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "ffc193e3-53d8-4d6a-fbfb-05d79b20c245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3232029878.py:6: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "  ax.boxplot(steps.values(), labels=steps.keys())\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create the box plot\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(steps.values(), labels=steps.keys())\n",
        "\n",
        "# Add titles and labels (optional)\n",
        "ax.set_title('Steps to 10 successes')\n",
        "ax.set_xlabel('Agent')\n",
        "ax.set_ylabel('Steps')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "M-qQL1MWzlpa",
        "_E1l6srHhpOa",
        "eOTXAQWqxWRd",
        "Wzw5lnXmzxrD",
        "HnR1TmjpiJaE"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f25a8f0b69f449e8acc20ad203e73b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf94eba0b41449029055af044667189a",
              "IPY_MODEL_e709dca5ae034706b813c8b355c03979",
              "IPY_MODEL_386938fd3db6410488d969922300edb7"
            ],
            "layout": "IPY_MODEL_e3c62854e742451f84239cd7224b03b6"
          }
        },
        "bf94eba0b41449029055af044667189a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8c9ca97b83e4b7db30e7e701618d781",
            "placeholder": "​",
            "style": "IPY_MODEL_1b57e19d26d14fb59326c9326a0ae008",
            "value": "Repetitions: 100%"
          }
        },
        "e709dca5ae034706b813c8b355c03979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07ae31987671426c973bb7e4df3309b6",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78f1e9b86cb94d4ea684c7a77b54a61d",
            "value": 30
          }
        },
        "386938fd3db6410488d969922300edb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23fd0658c7aa472db04e403774465313",
            "placeholder": "​",
            "style": "IPY_MODEL_5a1b706782ca410bb95e65d54720aa65",
            "value": " 30/30 [00:08&lt;00:00,  2.91it/s]"
          }
        },
        "e3c62854e742451f84239cd7224b03b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8c9ca97b83e4b7db30e7e701618d781": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b57e19d26d14fb59326c9326a0ae008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07ae31987671426c973bb7e4df3309b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78f1e9b86cb94d4ea684c7a77b54a61d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23fd0658c7aa472db04e403774465313": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a1b706782ca410bb95e65d54720aa65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}